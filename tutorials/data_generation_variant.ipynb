{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation for missense variants"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "<img style=\"margin-left: 1.5rem\" align=\"right\" src=\"images/data_generation_variants.png\" width=\"400\">\n",
    "\n",
    "This tutorial will demonstrate the use of DeepRank-Core for generating missense variants graphs and saving them as [HDF5 files](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) files, using [PBD files](https://en.wikipedia.org/wiki/Protein_Data_Bank_(file_format)) of protein structures as input.\n",
    "\n",
    "In this data processing phase, a local neighborhood around the mutated residue is selected for each variant according to a radius threshold that the user can customize. All atoms or residues within the threshold are mapped as the nodes to a graph and the interactions between them are the edges of the graph. Each node and edge can have several distinct (structural or physico-chemical) features, which are generated and added during the processing phase as well. Optionally, the graphs can be mapped to volumetric grids (i.e., 3D image-like representations). Finally, the mapped data are saved as HDF5 files, which can be used for training predictive models (for details see [training_ppi.ipynb](https://github.com/DeepRank/deeprank-core/blob/main/tutorials/training_ppi.ipynb) tutorial). In particular, graphs can be used for the training of Graph Neural Networks (GNNs), and grids can be used for the training of Convolutional Neural Networks (CNNs)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Data\n",
    "\n",
    "The example data used in this tutorial are available on Zenodo at [this record address](https://zenodo.org/record/8187806). To download the raw data used in this tutorial, please visit the link and download `data_raw.zip`. Unzip it, and save the `data_raw/` folder in the same directory as this notebook. The name and the location of the folder are optional but recommended, as they are the name and the location we will use to refer to the folder throughout the tutorial.\n",
    "\n",
    "Note that the dataset contains only 96 data points, which is not enough to develop an impactful predictive model, and the scope of its use is indeed only demonstrative and informative for the users."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The libraries needed for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import h5py\n",
    "import matplotlib.image as img\n",
    "import matplotlib.pyplot as plt\n",
    "from deeprankcore.query import QueryCollection\n",
    "from deeprankcore.query import SingleResidueVariantResidueQuery, SingleResidueVariantAtomicQuery\n",
    "from deeprankcore.domain.aminoacidlist import (alanine, arginine, asparagine,\n",
    "                                               serine, glycine, leucine, aspartate,\n",
    "                                               glutamine, glutamate, lysine, phenylalanine, histidine,\n",
    "                                               tyrosine, tryptophan, valine, proline,\n",
    "                                               cysteine, isoleucine, methionine, threonine)\n",
    "from deeprankcore.features import components, contact\n",
    "from deeprankcore.utils.grid import GridSettings, MapMethod\n",
    "from deeprankcore.dataset import GraphDataset\n",
    "\n",
    "aa_dict = {\"ALA\": alanine, \"CYS\": cysteine, \"ASP\": aspartate,\n",
    "           \"GLU\": glutamate, \"PHE\": phenylalanine, \"GLY\": glycine, \n",
    "           \"HIS\": histidine, \"ILE\": isoleucine, \"LYS\": lysine,\n",
    "           \"LEU\": leucine, \"MET\": methionine, \"ASN\": asparagine,\n",
    "           \"PRO\": proline, \"GLN\": glutamine, \"ARG\": arginine,\n",
    "           \"SER\": serine, \"THR\": threonine, \"VAL\": valine,\n",
    "           \"TRP\": tryptophan, \"TYR\": tyrosine\n",
    "           }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw files and paths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paths for reading raw data and saving the processed ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"data_raw\", \"variant\")\n",
    "processed_data_path = os.path.join(\"data_processed\", \"variant\")\n",
    "os.makedirs(os.path.join(processed_data_path, \"residue\"))\n",
    "os.makedirs(os.path.join(processed_data_path, \"atomic\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Raw data are PDB files in `data_raw/variant/pdb/`, which contains atomic coordinates of the protein structure containing the variant. \n",
    "- Target data, so in our case pathogenic versus benign labels, are in `data_raw/variant/variant_target_values.csv`.\n",
    "- The final variant processed data will be saved in `data_processed/variant/` folder, which in turns contains a folder for residue-level data and another one for atomic-level data. More details about such different levels will come a few cells below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_pdb_files_and_target_data` is an helper function used to retrieve the raw pdb files names, variants information and target values in a list from the CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdb_files_and_target_data(data_path):\n",
    "\tcsv_data = pd.read_csv(os.path.join(data_path, \"variant_target_values.csv\"))\n",
    "\tpdb_files = glob.glob(os.path.join(data_path, \"pdb\", '*.ent'))\n",
    "\tpdb_files.sort()\n",
    "\tpdb_file_names = [os.path.basename(pdb_file) for pdb_file in pdb_files]\n",
    "\tcsv_data_indexed = csv_data.set_index('pdb_file')\n",
    "\tcsv_data_indexed = csv_data_indexed.loc[pdb_file_names]\n",
    "\tres_numbers = csv_data_indexed.res_number.values.tolist()\n",
    "\tres_wildtypes = csv_data_indexed.res_wildtype.values.tolist()\n",
    "\tres_variants = csv_data_indexed.res_variant.values.tolist()\n",
    "\ttargets = csv_data_indexed.target.values.tolist()\n",
    "\tpdb_names = csv_data_indexed.index.values.tolist()\n",
    "\tpdb_files = [data_path + \"/pdb/\" + pdb_name for pdb_name in pdb_names]\n",
    "\treturn pdb_files, res_numbers, res_wildtypes, res_variants, targets\n",
    "\n",
    "pdb_files, res_numbers, res_wildtypes, res_variants, targets = get_pdb_files_and_target_data(data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `QueryCollection` and `Query` objects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each missense variant, so for each data point, a query can be created and added to the `QueryCollection` object, to be processed later on. Different types of queries exist, based on the molecular resolution needed:\n",
    "\n",
    "- In a `SingleResidueVariantResidueQuery` each node represents one amino acid residue.\n",
    "- In a `SingleResidueVariantAtomicQuery` each node represents one atom within the amino acid residues.\n",
    "\n",
    "A query takes as inputs:\n",
    "\n",
    "- A `.pdb` file, representing the protein structure containing the missense variant.\n",
    "- The chain id of the variant residue.\n",
    "- The residue number of the missense mutation.\n",
    "- The insertion code, ??\n",
    "- The wildtype amino acid. \n",
    "- The variant amino acid. \n",
    "- The radius, which determines the threshold distance (in Ångström) for residues/atoms surrounding the mutation that will be included in the graph.\n",
    "- The distance cutoff, which represents the maximum distance in Ångström between two interacting residues/atoms.\n",
    "- The target values associated with the query. For each query/data point, in the use case demonstrated in this tutorial will add a 0 if the variant belongs to the benign class, and 1 if it belongs to the pathogenic one. \n",
    "- Optional: The correspondent [Position-Specific Scoring Matrices (PSSMs)](https://en.wikipedia.org/wiki/Position_weight_matrix), per chain identifier, in the form of .pssm files. PSSMs are optional and will not be used in this tutorial."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residue-level variant: `SingleResidueVariantResidueQuery`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = QueryCollection()\n",
    "\n",
    "radius = 10.0 # radius to select the local neighborhood around the variant\n",
    "distance_cutoff = 4.5 # ??\n",
    "\n",
    "print(f'Adding {len(pdb_files)} queries to the query collection ...')\n",
    "count = 0\n",
    "for i in range(len(pdb_files)):\n",
    "\tqueries.add(SingleResidueVariantResidueQuery(\n",
    "\t\tpdb_path = pdb_files[i],\n",
    "\t\tchain_id = \"A\",\n",
    "\t\tresidue_number = res_numbers[i],\n",
    "\t\tinsertion_code = None,\n",
    "\t\twildtype_amino_acid = aa_dict[res_wildtypes[i]],\n",
    "\t\tvariant_amino_acid = aa_dict[res_variants[i]],\n",
    "\t\ttargets = {'binary': targets[i]},\n",
    "\t\tradius = radius,\n",
    "\t\tdistance_cutoff = distance_cutoff,\n",
    "\t\t))\n",
    "\tcount +=1\n",
    "\tif count % 20 == 0:\n",
    "\t\tprint(f'{count} queries added to the collection.')\n",
    "\n",
    "print(f'Queries ready to be processed.\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on `process()` method\n",
    "\n",
    "Once all queries have been added to the `QueryCollection` instance, they can be processed. Main parameters of the `process()` method, include:\n",
    "\n",
    "- `prefix` sets the output file location.\n",
    "- `feature_modules` allows you to choose which feature generating modules you want to use. By default, the basic features contained in `deeprankcore.features.components` and `deeprankcore.features.contact` are generated. Users can add custom features by creating a new module and placing it in the `deeprankcore.feature` subpackage. A complete and detailed list of the pre-implemented features per module and more information about how to add custom features can be found [here](https://deeprankcore.readthedocs.io/en/latest/features.html).\n",
    "  - Note that all features generated by a module will be added if that module was selected, and there is no way to only generate specific features from that module. However, during the training phase shown in `training_ppi.ipynb`, it is possible to select only a subset of available features.\n",
    "- `cpu_count` can be used to specify how many processes to be run simultaneously, and will coincide with the number of HDF5 files generated. By default it takes all available CPU cores and HDF5 files are squashed into a single file using the `combine_output` setting.\n",
    "- Optional: If you want to include grids in the HDF5 files, which represent the mapping of the graphs to a volumetric box, you need to define `grid_settings` and `grid_map_method`, as shown in the example below. If they are `None` (default), only graphs are saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_settings = GridSettings( # None if you don't want grids\n",
    "\t# the number of points on the x, y, z edges of the cube\n",
    "\tpoints_counts = [35, 30, 30],\n",
    "\t# x, y, z sizes of the box in Å\n",
    "\tsizes = [1.0, 1.0, 1.0])\n",
    "grid_map_method = MapMethod.GAUSSIAN # None if you don't want grids\n",
    "\n",
    "queries.process(\n",
    "\tprefix = os.path.join(processed_data_path, \"residue\", \"proc\"),\n",
    "\tfeature_modules = [components, contact],\n",
    "    cpu_count = 8,\n",
    "\tcombine_output = False,\n",
    "\tgrid_settings = grid_settings,\n",
    "\tgrid_map_method = grid_map_method)\n",
    "\n",
    "print(f'The queries processing is done. The generated HDF5 files are in {os.path.join(processed_data_path, \"residue\")}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring data\n",
    "\n",
    "As representative example, the following is the HDF5 structure generated by the previous code for `pdb2ooh.ent`, so for one single graph, which represents one protein structure containing a missense variant in position 112, for the graph + grid case:\n",
    "\n",
    "```bash\n",
    "└── residue-graph:A:112:Threonine->Isoleucine:pdb2ooh\n",
    "    |\n",
    "    ├── edge_features\n",
    "    │   ├── _index\n",
    "    │   ├── _name\n",
    "    │   ├── covalent\n",
    "    │   ├── distance\n",
    "    │   ├── electrostatic\n",
    "    │   ├── same_chain\n",
    "    │   └── vanderwaals\n",
    "    |\n",
    "    ├── node_features\n",
    "    │   ├── _chain_id\n",
    "    │   ├── _name\n",
    "    │   ├── _position\n",
    "    │   ├── diff_charge\n",
    "    │   ├── diff_hb_donors\n",
    "    │   ├── diff_hb_acceptors\n",
    "    │   ├── diff_mass\n",
    "    │   ├── diff_pI\n",
    "    │   ├── diff_polarity\n",
    "    │   ├── diff_size\n",
    "    │   ├── hb_acceptors\n",
    "    │   ├── hb_donors\n",
    "    │   ├── polarity\n",
    "    │   ├── res_charge\n",
    "    │   ├── res_mass\n",
    "    |   ├── res_pI\n",
    "    |   ├── res_size\n",
    "    |   ├── res_type\n",
    "    |   └── variant_res\n",
    "    |\n",
    "    ├── grid_points\n",
    "    │   ├── center\n",
    "    │   ├── x\n",
    "    │   ├── y\n",
    "    │   └── z\n",
    "    |\n",
    "    ├── mapped_features\n",
    "    │   ├── _position_000\n",
    "    │   ├── _position_001\n",
    "    │   ├── _position_002\n",
    "    │   ├── covalent\n",
    "    │   ├── distance\n",
    "    │   ├── electrostatic\n",
    "    │   ├── diff_polarity_000\n",
    "    │   ├── diff_polarity_001\n",
    "    │   ├── diff_polarity_002\n",
    "    │   ├── diff_polarity_003\n",
    "    |   ├── ...\n",
    "    |   └── vanderwaals\n",
    "    |\n",
    "    └── target_values\n",
    "        └── binary\n",
    "```\n",
    "\n",
    "`edge_features`, `node_features`, `mapped_features` are [HDF5 Groups](https://docs.h5py.org/en/stable/high/group.html) which contain [HDF5 Datasets](https://docs.h5py.org/en/stable/high/dataset.html) (e.g., `_index`, `electrostatic`, etc.), which in turn contains features values in the form of arrays. `edge_features` and `node_features` refer specificly to the graph representation, while `grid_points` and `mapped_features` refer to the grid mapped from the graph. Each data point generated by deeprankcore has the above structure, with the features and the target changing according to the user's settings. Features starting with `_` are present for human inspection of the data, but they are not used for training models.\n",
    "\n",
    "It is always a good practice to first explore the data, and then make decision about splitting them in training, test and validation sets. There are different possible ways for doing it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas dataframe\n",
    "\n",
    "The edge and node features just generated can be explored by instantiating the `GraphDataset` object, and then using `hdf5_to_pandas` method which converts node and edge features into a [Pandas](https://pandas.pydata.org/) dataframe. Each row represents a ppi in the form of a graph.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = glob.glob(os.path.join(processed_data_path, \"residue\", \"*.hdf5\"))\n",
    "dataset = GraphDataset(processed_data)\n",
    "df = dataset.hdf5_to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also generate histograms for looking at the features distributions. An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(processed_data_path, \"residue\", \"_\".join([\"res_mass\", \"distance\", \"electrostatic\"]))\n",
    "dataset.save_hist(\n",
    "    features = [\"res_mass\", \"distance\", \"electrostatic\"],\n",
    "    fname = fname)\n",
    "\n",
    "im = img.imread(fname + \".png\")\n",
    "plt.figure(figsize = (15,10))\n",
    "fig = plt.imshow(im)\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other tools\n",
    "\n",
    "- [HDFView](https://www.hdfgroup.org/downloads/hdfview/), a visual tool written in Java for browsing and editing HDF5 files.\n",
    "  As representative example, the following is the structure for `pdb2ooh.ent` seen from HDF5View:\n",
    "  \n",
    "  <img style=\"margin-bottom: 1.5rem\" align=\"centrum\" src=\"images/hdfview_variant.png\" width=\"200\">\n",
    "\n",
    "  Using this tool you can inspect the values of the features visually, for each data point. \n",
    "\n",
    "- Python packages such as [h5py](https://docs.h5py.org/en/stable/index.html). Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(processed_data[0], \"r\") as hdf5:\n",
    "    # List of all graphs in hdf5, each graph representing\n",
    "    # a missense variant and its sourrouding environment\n",
    "    ids = list(hdf5.keys())\n",
    "    print(f'IDs of missense variants in {processed_data[0]}: {ids}')\n",
    "    node_features = list(hdf5[ids[0]][\"node_features\"]) \n",
    "    print(f'Node features: {node_features}')\n",
    "    edge_features = list(hdf5[ids[0]][\"edge_features\"])\n",
    "    print(f'Edge features: {edge_features}')\n",
    "    target_features = list(hdf5[ids[0]][\"target_values\"])\n",
    "    print(f'Targets features: {target_features}')\n",
    "    # Polarity feature for ids[0], numpy.ndarray\n",
    "    node_feat_polarity = hdf5[ids[0]][\"node_features\"][\"polarity\"][:]\n",
    "    print(f'Polarity feature shape: {node_feat_polarity.shape}')\n",
    "    # Electrostatic feature for ids[0], numpy.ndarray\n",
    "    edge_feat_electrostatic = hdf5[ids[0]][\"edge_features\"][\"electrostatic\"][:]\n",
    "    print(f'Electrostatic feature shape: {edge_feat_electrostatic.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atomic-level variant: `SingleResidueVariantAtomicQuery`\n",
    "\n",
    "Graphs can also be generated at an atomic resolution, very similarly to what has just been done for residue-level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = QueryCollection()\n",
    "\n",
    "radius = 10.0 # radius to select the local neighborhood around the variant\n",
    "distance_cutoff = 4.5 # ??\n",
    "\n",
    "print(f'Adding {len(pdb_files)} queries to the query collection ...')\n",
    "count = 0\n",
    "for i in range(len(pdb_files)):\n",
    "\tqueries.add(SingleResidueVariantAtomicQuery(\n",
    "\t\tpdb_path = pdb_files[i],\n",
    "\t\tchain_id = \"A\",\n",
    "\t\tresidue_number = res_numbers[i],\n",
    "\t\tinsertion_code = None,\n",
    "\t\twildtype_amino_acid = aa_dict[res_wildtypes[i]],\n",
    "\t\tvariant_amino_acid = aa_dict[res_variants[i]],\n",
    "\t\ttargets = {'binary': targets[i]},\n",
    "\t\tradius = radius,\n",
    "\t\tdistance_cutoff = distance_cutoff,\n",
    "\t\t))\n",
    "\tcount +=1\n",
    "\tif count % 20 == 0:\n",
    "\t\tprint(f'{count} queries added to the collection.')\n",
    "\n",
    "print(f'Queries ready to be processed.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_settings = GridSettings( # None if you don't want grids\n",
    "\t# the number of points on the x, y, z edges of the cube\n",
    "\tpoints_counts = [35, 30, 30],\n",
    "\t# x, y, z sizes of the box in Å\n",
    "\tsizes = [1.0, 1.0, 1.0])\n",
    "grid_map_method = MapMethod.GAUSSIAN # None if you don't want grids\n",
    "\n",
    "queries.process(\n",
    "\tprefix = os.path.join(processed_data_path, \"atomic\", \"proc\"),\n",
    "\tfeature_modules = [components, contact],\n",
    "    cpu_count = 8,\n",
    "\tcombine_output = False,\n",
    "\tgrid_settings = grid_settings,\n",
    "\tgrid_map_method = grid_map_method)\n",
    "\n",
    "print(f'The queries processing is done. The generated HDF5 files are in {os.path.join(processed_data_path, \"atomic\")}.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the data can be inspected using `hdf5_to_pandas` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = glob.glob(os.path.join(processed_data_path, \"atomic\", \"*.hdf5\"))\n",
    "dataset = GraphDataset(processed_data)\n",
    "df = dataset.hdf5_to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join(processed_data_path, \"atomic\", \"atom_charge\")\n",
    "dataset.save_hist(\n",
    "    features = \"atom_charge\",\n",
    "    fname = fname)\n",
    "\n",
    "im = img.imread(fname + \".png\")\n",
    "plt.figure(figsize = (8,8))\n",
    "fig = plt.imshow(im)\n",
    "fig.axes.get_xaxis().set_visible(False)\n",
    "fig.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the features are different from the ones generated with the residue-level queries. There are indeed features in `deeprankcore.features.components` module which are generated only in atomic graphs, i.e. `atom_type`, `atom_charge`, and `pdb_occupancy`, because they don't make sense only in the atomic graphs' representation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprankcore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
