{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepRank-Core basic Protein-Protein Interface training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example data used in this tutorial are available on Zenodo at [this record address](https://zenodo.org/record/7997586). To download the data, please visit the link and click the â€œDownload\" button. Unzip the downloaded file, and save the contents as a folder named `data/` in the same directory as this notebook. The name data and the folder location are optional but recommended, as this are the name and the location we will use to refer to the folder throughout the tutorial.\n",
    "\n",
    "The dataset contains only 100 data points, which are obviously not enough to develop an impactful predictive model, and the scope of its use is indeed only demonstrative and informative for the users."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Follow the [updated instructions](https://github.com/DeepRank/deeprank-core#installation) in the README.md on the main branch for successfully installing `deeprankcore` package.\n",
    "2. To test the environment in which` deeprankcore` has been (successfully) installed, first clone [deeprank-core repository](https://github.com/DeepRank/deeprank-core). Navigate into it, and after having activated the environment and installed [pytest](https://anaconda.org/anaconda/pytest), run `pytest tests`. All tests should pass. We recommend installing `deeprankcore` and all its dependencies into a [conda](https://docs.conda.io/en/latest/) environment.\n",
    "3. Additionally, for this tutorial you need to install [scikit-learn](https://scikit-learn.org/stable/install.html) and [plotly](https://anaconda.org/plotly/plotly)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"margin-left: 1.5rem\" align=\"right\" src=\"images/training_ppi.png\" width=\"400\">\n",
    "\n",
    "This tutorial will demonstrate the use of DeepRank-Core for training Graph Neural Networks (GNNs) and Convolutional Neural Networks (CNNs) using Protein-Protein Interface (PPI) data for classification and regression predictive tasks.\n",
    "\n",
    "This tutorial assumes that the PPI data of interest have already been generated and saved into [HDF5 files](https://en.wikipedia.org/wiki/Hierarchical_Data_Format), with the data structure that DeepRank-Core expects. Such data have already been generated in `data/ppi/processed/` folder, but for more details about the process please refer to the tutorial notebook [data_generation_ppi.ipynb](https://github.com/DeepRank/deeprank-core/blob/main/tutorials/data_generation_ppi.ipynb). It contains a detailed description of how such data are generated starting from the PDB files and gives instructions for generating different resolutions' data, i.e. residue- and atomic-level.\n",
    "\n",
    "This tutorial assumes also a basic knowledge of the [PyTorch](https://pytorch.org/) framework, on top of which the machine learning pipeline of DeepRank-Core has been developed. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"margin-right: 1.5rem\" align=\"left\" src=\"images/pmhc_pdb_example.png\" width=\"200\"/>\n",
    "\n",
    "The example dataset that we provide contains PDB files representing [Major Histocompatibility Complex (MHC) protein](https://en.wikipedia.org/wiki/Major_histocompatibility_complex) + peptide (pMHC) complexes, which play a key role in T-cell immunity. We are interested in predicting the Binding Affinity (BA) of the complexes, which can be used to determine the most suitable mutated tumor peptides as vaccine candidates.\n",
    "\n",
    "PDB models used in this tutorial have been generated with [PANDORA](https://github.com/X-lab-3D/PANDORA), an anchor restrained modeling pipeline for generating peptide-MHC structures. While target data, so the BA values for such pMHC complexes, have been retrieved from [MHCFlurry 2.0](https://data.mendeley.com/datasets/zx3kjzc3yx).\n",
    "\n",
    "On the left an example of a pMHC structure is shown, rendered using [ProteinViewer](https://marketplace.visualstudio.com/items?itemName=ArianJamasb.protein-viewer). The MHC protein is displayed in green, while the peptide is in orange."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the libraries needed for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    f1_score)\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import numpy as np\n",
    "np.seterr(divide = 'ignore')\n",
    "np.seterr(invalid='ignore')\n",
    "import pandas as pd\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from deeprankcore.dataset import GraphDataset, GridDataset\n",
    "from deeprankcore.trainer import Trainer\n",
    "from deeprankcore.neuralnets.gnn.naive_gnn import NaiveNetwork\n",
    "from deeprankcore.neuralnets.cnn.model3d import CnnClassification\n",
    "from deeprankcore.utils.exporters import HDF5OutputExporter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths and sets\n",
    "\n",
    "Let's define the paths for reading the processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"residue\"\n",
    "processed_data_path = os.path.join(\"data\", \"ppi\", \"processed\", level)\n",
    "input_data_path = glob.glob(os.path.join(processed_data_path, '*.hdf5'))\n",
    "output_path = os.path.join(\"data\", \"ppi\") # for saving predictions results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The levels refer to a different molecular resolution, either residue- or atomic-level. In residue-level graphs, each node represents one amino acid residue, while in atomic-level graphs each node represents one atom within the amino acid residues. In this tutorial, we will use residue-level data, but the same code could be applied to atomic-level data with no changes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a Pandas dataframe containing data points' IDs and the binary target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>residue-ppi:M-P:BA-102611</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>residue-ppi:M-P:BA-102669</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>residue-ppi:M-P:BA-102719</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>residue-ppi:M-P:BA-114468</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>residue-ppi:M-P:BA-115138</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       entry  target\n",
       "0  residue-ppi:M-P:BA-102611     1.0\n",
       "1  residue-ppi:M-P:BA-102669     0.0\n",
       "2  residue-ppi:M-P:BA-102719     0.0\n",
       "3  residue-ppi:M-P:BA-114468     0.0\n",
       "4  residue-ppi:M-P:BA-115138     0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict = {}\n",
    "df_dict['entry'] = []\n",
    "df_dict['target'] = []\n",
    "for fname in input_data_path:\n",
    "    with h5py.File(fname, 'r') as hdf5:\n",
    "        for mol in hdf5.keys():\n",
    "            target_value = float(hdf5[mol][\"target_values\"][\"binary\"][()])\n",
    "            df_dict['entry'].append(mol)\n",
    "            df_dict['target'].append(target_value)\n",
    "\n",
    "df = pd.DataFrame(data=df_dict)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in [data_generation_ppi.ipynb](https://github.com/DeepRank/deeprank-core/blob/main/tutorials/data_generation_ppi.ipynb), for each data point there are two targets: \"BA\" and \"binary\". The first represents the continuous Binding Affinity value of the complex, while the second represents its binary representation, being 0 (BA > 500 nM) a not-binding complex and 1 (BA <= 500 nM) binding one.\n",
    "\n",
    "The dataframe `df` is used only to split data points into training, validation and test sets according to the \"binary\" target - using the target stratification, to keep the proportion of 0s and 1s sort of constant among the different sets. Training and validation set will be used during the training for updating the netowork's weights, while the test set will be held out as an indipendent test and will be used for later model's evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data statistics:\n",
      "\n",
      "Total samples: 100\n",
      "\n",
      "Training set: 72 samples, 72%\n",
      "\t- Class 0: 36 samples, 50%\n",
      "\t- Class 1: 36 samples, 50%\n",
      "Validation set: 18 samples, 18%\n",
      "\t- Class 0: 9 samples, 50%\n",
      "\t- Class 1: 9 samples, 50%\n",
      "Testing set: 10 samples, 10%\n",
      "\t- Class 0: 5 samples, 50%\n",
      "\t- Class 1: 5 samples, 50%\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.1, stratify=df.target, random_state=42)\n",
    "df_train, df_valid = train_test_split(df_train, test_size=0.2, stratify=df_train.target, random_state=42)\n",
    "\n",
    "print(f'Data statistics:\\n')\n",
    "print(f'Total samples: {len(df)}\\n')\n",
    "print(f'Training set: {len(df_train)} samples, {round(100*len(df_train)/len(df))}%')\n",
    "print(f'\\t- Class 0: {len(df_train[df_train.target == 0])} samples, {round(100*len(df_train[df_train.target == 0])/len(df_train))}%')\n",
    "print(f'\\t- Class 1: {len(df_train[df_train.target == 1])} samples, {round(100*len(df_train[df_train.target == 1])/len(df_train))}%')\n",
    "print(f'Validation set: {len(df_valid)} samples, {round(100*len(df_valid)/len(df))}%')\n",
    "print(f'\\t- Class 0: {len(df_valid[df_valid.target == 0])} samples, {round(100*len(df_valid[df_valid.target == 0])/len(df_valid))}%')\n",
    "print(f'\\t- Class 1: {len(df_valid[df_valid.target == 1])} samples, {round(100*len(df_valid[df_valid.target == 1])/len(df_valid))}%')\n",
    "print(f'Testing set: {len(df_test)} samples, {round(100*len(df_test)/len(df))}%')\n",
    "print(f'\\t- Class 0: {len(df_test[df_test.target == 0])} samples, {round(100*len(df_test[df_test.target == 0])/len(df_test))}%')\n",
    "print(f'\\t- Class 1: {len(df_test[df_test.target == 1])} samples, {round(100*len(df_test[df_test.target == 1])/len(df_test))}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification example\n",
    "\n",
    "Let's train a GNN and a CNN for a classification predictive task, which consists in predicting the \"binary\" target values. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GraphDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training GNNs the user can create `GraphDataset` instances. The latter class inherits from `DeeprankDataset` class, which in turns inherits from `Dataset` [PyTorch geometric class](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/data/dataset.html), a base class for creating graph datasets.\n",
    "\n",
    "A few notes about `GraphDataset` parameters:\n",
    "- By default, all features contained in the HDF5 files are used, but the user can specify `node_features` and `edge_features` in `GraphDataset` if not all of them are needed. See the [docs](https://deeprankcore.readthedocs.io/en/latest/features.html) for more details about all the possible pre-implemented features. \n",
    "- For the `GraphDataset` class only it is possible to define a dictionary to indicate which transformations to apply to the features, being the transformations lambda functions and/or standardization. If `True`, standardization is applied after transformation, if the latter is present. Standardization consists in applying the following formula on each feature's value: ${x' = \\frac{x - \\mu}{\\sigma}}$, being ${\\mu}$ the mean and ${\\sigma}$ the standard deviation. Standardization is a scaling method where the values are centered around mean with a unit standard deviation. In the example below we will apply a logarithmic transformation and then standardization to all the features. It is also possible to use specific features as keys for indicating that we want to apply transformation and/or standardization to few features only. \n",
    "- Since we are applying standardization, we need to use training features' means and standard deviations to scale validation and test sets. For doing so, `train` and `dataset_train` parameters are used. When `train` is set `False`, a `dataset_train` of the same class must be provided and it will be used to scale the validation/testing set according to its features values. You need to pass `features_transform` to the training dataset only, since in other cases it will be ignored and only the one of `dataset_train` will be considered. \n",
    "- For regression, `task` can be assigned to `regress` and the `target` to `BA`, which is a continuous variable and this it is suitable for regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.dataset:\n",
      "Checking dataset Integrity...\n",
      "INFO:deeprankcore.dataset:Target classes set up to: [0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "   ['data/ppi/processed/residue/proc-28835.hdf5', 'data/ppi/processed/residue/proc-28842.hdf5', 'data/ppi/processed/residue/proc-28839.hdf5', 'data/ppi/processed/residue/proc-28838.hdf5', 'data/ppi/processed/residue/proc-28843.hdf5', 'data/ppi/processed/residue/proc-28834.hdf5', 'data/ppi/processed/residue/proc-28837.hdf5', 'data/ppi/processed/residue/proc-28840.hdf5', 'data/ppi/processed/residue/proc-28841.hdf5', 'data/ppi/processed/residue/proc-28836.hdf5'] dataset                 : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 403.02it/s, entry_name=proc-28836.hdf5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.dataset:\n",
      "Checking dataset Integrity...\n",
      "INFO:deeprankcore.dataset:Target classes set up to: [0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading validation data...\n",
      "   ['data/ppi/processed/residue/proc-28835.hdf5', 'data/ppi/processed/residue/proc-28842.hdf5', 'data/ppi/processed/residue/proc-28839.hdf5', 'data/ppi/processed/residue/proc-28838.hdf5', 'data/ppi/processed/residue/proc-28843.hdf5', 'data/ppi/processed/residue/proc-28834.hdf5', 'data/ppi/processed/residue/proc-28837.hdf5', 'data/ppi/processed/residue/proc-28840.hdf5', 'data/ppi/processed/residue/proc-28841.hdf5', 'data/ppi/processed/residue/proc-28836.hdf5'] dataset                 : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 728.01it/s, entry_name=proc-28836.hdf5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:deeprankcore.dataset:features_transform parameter, if set, will be ignored.\n",
      "features_transform will remain the same as the one used in training phase.\n",
      "INFO:deeprankcore.dataset:\n",
      "Checking dataset Integrity...\n",
      "INFO:deeprankcore.dataset:Target classes set up to: [0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading test data...\n",
      "   ['data/ppi/processed/residue/proc-28835.hdf5', 'data/ppi/processed/residue/proc-28842.hdf5', 'data/ppi/processed/residue/proc-28839.hdf5', 'data/ppi/processed/residue/proc-28838.hdf5', 'data/ppi/processed/residue/proc-28843.hdf5', 'data/ppi/processed/residue/proc-28834.hdf5', 'data/ppi/processed/residue/proc-28837.hdf5', 'data/ppi/processed/residue/proc-28840.hdf5', 'data/ppi/processed/residue/proc-28841.hdf5', 'data/ppi/processed/residue/proc-28836.hdf5'] dataset                 : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 805.93it/s, entry_name=proc-28836.hdf5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:deeprankcore.dataset:features_transform parameter, if set, will be ignored.\n",
      "features_transform will remain the same as the one used in training phase.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target = \"binary\"\n",
    "task = \"classif\"\n",
    "features_transform = {'all': {'transform': lambda x: np.cbrt(x), 'standardize': True}}\n",
    "\n",
    "print('Loading training data...')\n",
    "dataset_train = GraphDataset(\n",
    "    hdf5_path = input_data_path,\n",
    "    subset = list(df_train.entry), # selects only data points with ids in df_train.entry\n",
    "    target = target,\n",
    "    task = task,\n",
    "    features_transform = features_transform\n",
    ")\n",
    "print('\\nLoading validation data...')\n",
    "dataset_val = GraphDataset(\n",
    "    hdf5_path = input_data_path,\n",
    "    subset = list(df_valid.entry), # selects only data points with ids in df_valid.entry\n",
    "    target = target,\n",
    "    task = task,\n",
    "    train = False,\n",
    "    dataset_train = dataset_train\n",
    ")\n",
    "print('\\nLoading test data...')\n",
    "dataset_test = GraphDataset(\n",
    "    hdf5_path = input_data_path,\n",
    "    subset = list(df_test.entry), # selects only data points with ids in df_test.entry\n",
    "    target = target,\n",
    "    task = task,\n",
    "    train = False,\n",
    "    dataset_train = dataset_train\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer\n",
    "\n",
    "The class `Trainer` implements training, validation and testing of PyTorch-based neural networks. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes about `Trainer` parameters:\n",
    "- `neuralnet` can be whatever neural network class which inherits from `torch.nn.Module`, and it shouldn't be specific to regression or classification in terms of output shape. `Trainer` class takes care of formatting the output shape according to the task. In this tutorial we will use `NaiveNetwork` (in `deeprankcore.neuralnets.gnn.naive_gnn`), whose architecture is shown below:\n",
    "  ```python\n",
    "  class NaiveConvolutionalLayer(Module):\n",
    "\n",
    "    def __init__(self, count_node_features, count_edge_features):\n",
    "        super().__init__()\n",
    "        message_size = 32\n",
    "        edge_input_size = 2 * count_node_features + count_edge_features\n",
    "        self._edge_mlp = Sequential(Linear(edge_input_size, message_size), ReLU())\n",
    "        node_input_size = count_node_features + message_size\n",
    "        self._node_mlp = Sequential(Linear(node_input_size, count_node_features), ReLU())\n",
    "\n",
    "    def forward(self, node_features, edge_node_indices, edge_features):\n",
    "        # generate messages over edges\n",
    "        node0_indices, node1_indices = edge_node_indices\n",
    "        node0_features = node_features[node0_indices]\n",
    "        node1_features = node_features[node1_indices]\n",
    "        message_input = torch.cat([node0_features, node1_features, edge_features], dim=1)\n",
    "        messages_per_neighbour = self._edge_mlp(message_input)\n",
    "        # aggregate messages\n",
    "        out = torch.zeros(node_features.shape[0], messages_per_neighbour.shape[1]).to(node_features.device)\n",
    "        message_sums_per_node = scatter_sum(messages_per_neighbour, node0_indices, dim=0, out=out)\n",
    "        # update nodes\n",
    "        node_input = torch.cat([node_features, message_sums_per_node], dim=1)\n",
    "        node_output = self._node_mlp(node_input)\n",
    "        return node_output\n",
    "\n",
    "    class NaiveNetwork(Module):\n",
    "\n",
    "        def __init__(self, input_shape: int, output_shape: int, input_shape_edge: int):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                input_shape (int): Number of node input features.\n",
    "                output_shape (int): Number of output value per graph.\n",
    "                input_shape_edge (int): Number of edge input features.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self._external1 = NaiveConvolutionalLayer(input_shape, input_shape_edge)\n",
    "            self._external2 = NaiveConvolutionalLayer(input_shape, input_shape_edge)\n",
    "            hidden_size = 128\n",
    "            self._graph_mlp = Sequential(Linear(input_shape, hidden_size), ReLU(), Linear(hidden_size, output_shape))\n",
    "\n",
    "        def forward(self, data):\n",
    "            external_updated1_node_features = self._external1(data.x, data.edge_index, data.edge_attr)\n",
    "            external_updated2_node_features = self._external2(external_updated1_node_features, data.edge_index, data.edge_attr)\n",
    "            means_per_graph_external = scatter_mean(external_updated2_node_features, data.batch, dim=0)\n",
    "            graph_input = means_per_graph_external\n",
    "            z = self._graph_mlp(graph_input)\n",
    "            return z\n",
    "  ```\n",
    "- `class_weights` is used in classification tasks only, to assign class weights based on the training dataset content. In this case we have a balanced dataset (50% 0 and 50% 1), so it doesn't make sense to use it. It defaults to False. \n",
    "- `cuda` and `ngpu` are used for indicating whether to use CUDA and how many GPUs. By default, CUDA is not used and `ngpu` is 0.\n",
    "- The user can specify a deeprankcore exporter or a custom one in `output_exporters` parameter, together with the path where to save the results. Exporters are used for storing predictions information collected later on during training and testing. We will see later how to read the results saved by `HDF5OutputExporter`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.trainer:Device set to cpu.\n",
      "INFO:deeprankcore.trainer:No loss function provided, the default loss function for classif tasks is used: <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    neuralnet = NaiveNetwork,\n",
    "    dataset_train = dataset_train,\n",
    "    dataset_val = dataset_val,\n",
    "    dataset_test = dataset_test,\n",
    "    output_exporters = [HDF5OutputExporter(os.path.join(output_path, \"gnn_classif\"))]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default optimizer is `torch.optim.Adam`. It is possible to specify optimizer's parameters or to use another PyTorch optimizer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD\n",
    "lr = 1e-3\n",
    "weight_decay = 0.001\n",
    "\n",
    "trainer.configure_optimizers(optimizer, lr, weight_decay)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default loss function for classification is `torch.nn.CrossEntropyLoss`, while for regression is `torch.nn.MSELoss`. It is also possible to set other PyTorch loss functions by using `Trainer.set_lossfunction` method.\n",
    "\n",
    "Then we can train our model, using the `train()` method of the `Trainer` class.\n",
    "\n",
    "A few notes about `train()` method parameters:\n",
    "- `earlystop_patience`, `earlystop_maxgap` and `min_epoch` are used for controlling early stopping logic. `earlystop_patience` indicates the number of epochs after which the training ends if the validation loss as not improved. `earlystop_maxgap` indicated the maximum difference allowed between validation and training loss, and `min_epoch` is the minimum epoch to be reached before looking at maxgap.\n",
    "- `validate` is set True to perform validation on an independent dataset, which we called `dataset_val` few cells above.\n",
    "- `num_workers` can be set for indicating how many subprocesses to use for data loading. The default is 0 and it means that the data will be loaded in the main process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.trainer:Training set loaded\n",
      "\n",
      "INFO:deeprankcore.trainer:Validation set loaded\n",
      "\n",
      "INFO:deeprankcore.trainer:Epoch 0:\n",
      "INFO:deeprankcore.trainer:training loss 0.8239355815781487 | time 0.4840049743652344\n",
      "INFO:deeprankcore.trainer:validation loss 0.750345379114151 | time 0.13220000267028809\n",
      "INFO:deeprankcore.trainer:Epoch 1:\n",
      "INFO:deeprankcore.trainer:training loss 4.725361492898729 | time 1.5999512672424316\n",
      "INFO:deeprankcore.trainer:validation loss 1.005086514684889 | time 0.1488018035888672\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 1.\n",
      "INFO:deeprankcore.trainer:Epoch 2:\n",
      "INFO:deeprankcore.trainer:training loss 0.835904081662496 | time 1.2459933757781982\n",
      "INFO:deeprankcore.trainer:validation loss 0.6674932440121969 | time 0.15125608444213867\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 2.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (1.005087 --> 0.667493).\n",
      "INFO:deeprankcore.trainer:Epoch 3:\n",
      "INFO:deeprankcore.trainer:training loss 0.7002725137604607 | time 1.1074721813201904\n",
      "INFO:deeprankcore.trainer:validation loss 0.6749191946453519 | time 0.18881487846374512\n",
      "INFO:deeprankcore.trainer:Validation loss did not decrease (0.667493 --> 0.674919). EarlyStopping counter: 1 out of 5\n",
      "INFO:deeprankcore.trainer:Epoch 4:\n",
      "INFO:deeprankcore.trainer:training loss 0.7429034643703036 | time 1.1848030090332031\n",
      "INFO:deeprankcore.trainer:validation loss 0.7832124961747063 | time 0.123931884765625\n",
      "INFO:deeprankcore.trainer:Validation loss did not decrease (0.667493 --> 0.783212). EarlyStopping counter: 2 out of 5\n",
      "INFO:deeprankcore.trainer:Epoch 5:\n",
      "INFO:deeprankcore.trainer:training loss 0.6848171154657999 | time 0.9921329021453857\n",
      "INFO:deeprankcore.trainer:validation loss 0.6418214109208848 | time 0.11592578887939453\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 5.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.667493 --> 0.641821).\n",
      "INFO:deeprankcore.trainer:Epoch 6:\n",
      "INFO:deeprankcore.trainer:training loss 0.7747999562157525 | time 0.9819679260253906\n",
      "INFO:deeprankcore.trainer:validation loss 0.648163186179267 | time 0.10247087478637695\n",
      "INFO:deeprankcore.trainer:Validation loss did not decrease (0.641821 --> 0.648163). EarlyStopping counter: 1 out of 5\n",
      "INFO:deeprankcore.trainer:Epoch 7:\n",
      "INFO:deeprankcore.trainer:training loss 0.661786675453186 | time 1.4448280334472656\n",
      "INFO:deeprankcore.trainer:validation loss 0.6712625556521945 | time 0.14447689056396484\n",
      "INFO:deeprankcore.trainer:Validation loss did not decrease (0.641821 --> 0.671263). EarlyStopping counter: 2 out of 5\n",
      "INFO:deeprankcore.trainer:Epoch 8:\n",
      "INFO:deeprankcore.trainer:training loss 0.7275134060117934 | time 1.3847191333770752\n",
      "INFO:deeprankcore.trainer:validation loss 0.8038990828726027 | time 0.18015003204345703\n",
      "INFO:deeprankcore.trainer:Validation loss did not decrease (0.641821 --> 0.803899). EarlyStopping counter: 3 out of 5\n",
      "INFO:deeprankcore.trainer:Epoch 9:\n",
      "INFO:deeprankcore.trainer:training loss 0.714947846200731 | time 1.4686899185180664\n",
      "INFO:deeprankcore.trainer:validation loss 0.6435654958089193 | time 0.1153409481048584\n",
      "INFO:deeprankcore.trainer:Validation loss did not decrease (0.641821 --> 0.643565). EarlyStopping counter: 4 out of 5\n",
      "INFO:deeprankcore.trainer:Epoch 10:\n",
      "INFO:deeprankcore.trainer:training loss 0.681432220670912 | time 1.1707429885864258\n",
      "INFO:deeprankcore.trainer:validation loss 0.653283695379893 | time 0.12291193008422852\n",
      "INFO:deeprankcore.trainer:Validation loss did not decrease (0.641821 --> 0.653284). EarlyStopping counter: 5 out of 5\n",
      "INFO:deeprankcore.trainer:EarlyStopping activated at epoch # 10 because patience of 5 has been reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at epoch 5\n",
      "Total # of parameters: 12230\n",
      "Total # of trainable parameters: 12230\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 8\n",
    "earlystop_patience = 5\n",
    "earlystop_maxgap = 0.1\n",
    "min_epoch = 10\n",
    "\n",
    "trainer.train(\n",
    "    nepoch = epochs,\n",
    "    batch_size = batch_size,\n",
    "    earlystop_patience = earlystop_patience,\n",
    "    earlystop_maxgap = earlystop_maxgap,\n",
    "    min_epoch = min_epoch,\n",
    "    validate = True,\n",
    "    filename = os.path.join(output_path, \"gnn_classif\", \"model.pth.tar\"))\n",
    "\n",
    "epoch = trainer.epoch_saved_model\n",
    "print(f\"Model saved at epoch {epoch}\")\n",
    "pytorch_total_params = sum(p.numel() for p in trainer.model.parameters())\n",
    "print(f'Total # of parameters: {pytorch_total_params}')\n",
    "pytorch_trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
    "print(f'Total # of trainable parameters: {pytorch_trainable_params}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing\n",
    "\n",
    "And we can test the trained model on our `dataset_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.trainer:Loading independent testing dataset...\n",
      "INFO:deeprankcore.trainer:Testing set loaded\n",
      "\n",
      "INFO:deeprankcore.trainer:testing loss 0.7380596995353699 | time 0.1068108081817627\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results visualization\n",
    "\n",
    "Finally, we can inspect the results saved by `HDF5OutputExporter`, which can be found in `data/ppi/gnn_classif/` folder in the form of an HDF5 file, `output_exporter.hdf5`. Note that the folder contains the saved pre-trained model as well. \n",
    "\n",
    "`output_exporter.hdf5` contains [HDF5 Groups](https://docs.h5py.org/en/stable/high/group.html) which refer to each phase, e.g. training and testing if both are run, only one of them otherwise. Training phase includes validation results as well. This HDF5 file can be read as a Pandas Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase</th>\n",
       "      <th>epoch</th>\n",
       "      <th>entry</th>\n",
       "      <th>output</th>\n",
       "      <th>target</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-116429</td>\n",
       "      <td>[0.7257537841796875, 0.2742462158203125]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.823936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-116810</td>\n",
       "      <td>[0.49615606665611267, 0.5038439631462097]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.823936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-135488</td>\n",
       "      <td>[0.6636247038841248, 0.33637526631355286]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.823936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-153607</td>\n",
       "      <td>[0.8759755492210388, 0.12402446568012238]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.823936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-119169</td>\n",
       "      <td>[0.6754884719848633, 0.3245115578174591]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.823936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phase  epoch                      entry  \\\n",
       "0  training    0.0  residue-ppi:M-P:BA-116429   \n",
       "1  training    0.0  residue-ppi:M-P:BA-116810   \n",
       "2  training    0.0  residue-ppi:M-P:BA-135488   \n",
       "3  training    0.0  residue-ppi:M-P:BA-153607   \n",
       "4  training    0.0  residue-ppi:M-P:BA-119169   \n",
       "\n",
       "                                      output  target      loss  \n",
       "0   [0.7257537841796875, 0.2742462158203125]     1.0  0.823936  \n",
       "1  [0.49615606665611267, 0.5038439631462097]     0.0  0.823936  \n",
       "2  [0.6636247038841248, 0.33637526631355286]     1.0  0.823936  \n",
       "3  [0.8759755492210388, 0.12402446568012238]     0.0  0.823936  \n",
       "4   [0.6754884719848633, 0.3245115578174591]     0.0  0.823936  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_train = pd.read_hdf(os.path.join(output_path, \"gnn_classif\", \"output_exporter.hdf5\"), key=\"training\")\n",
    "output_test = pd.read_hdf(os.path.join(output_path, \"gnn_classif\", \"output_exporter.hdf5\"), key=\"testing\")\n",
    "output_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframes contain `phase`, `epoch`, `entry`, `output`, `target`, and `loss` columns, and can be easily used to visualize the results.\n",
    "\n",
    "Let's plot for example the loss across the epochs for the training and the validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "phase=training<br>epoch=%{x}<br>loss=%{y}<extra></extra>",
         "legendgroup": "training",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "markers+lines",
         "name": "training",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10
         ],
         "xaxis": "x",
         "y": [
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          0.8239355815781487,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          4.725361492898729,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.835904081662496,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7002725137604607,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.7429034643703036,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.6848171154657999,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.7747999562157525,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.661786675453186,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.7275134060117934,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.714947846200731,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912,
          0.681432220670912
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "phase=validation<br>epoch=%{x}<br>loss=%{y}<extra></extra>",
         "legendgroup": "validation",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "markers+lines",
         "name": "validation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10
         ],
         "xaxis": "x",
         "y": [
          0.750345379114151,
          0.750345379114151,
          0.750345379114151,
          0.750345379114151,
          0.750345379114151,
          0.750345379114151,
          0.750345379114151,
          0.750345379114151,
          0.750345379114151,
          0.750345379114151,
          0.750345379114151,
          0.750345379114151,
          0.750345379114151,
          0.750345379114151,
          0.750345379114151,
          0.750345379114151,
          0.750345379114151,
          0.750345379114151,
          1.005086514684889,
          1.005086514684889,
          1.005086514684889,
          1.005086514684889,
          1.005086514684889,
          1.005086514684889,
          1.005086514684889,
          1.005086514684889,
          1.005086514684889,
          1.005086514684889,
          1.005086514684889,
          1.005086514684889,
          1.005086514684889,
          1.005086514684889,
          1.005086514684889,
          1.005086514684889,
          1.005086514684889,
          1.005086514684889,
          0.6674932440121969,
          0.6674932440121969,
          0.6674932440121969,
          0.6674932440121969,
          0.6674932440121969,
          0.6674932440121969,
          0.6674932440121969,
          0.6674932440121969,
          0.6674932440121969,
          0.6674932440121969,
          0.6674932440121969,
          0.6674932440121969,
          0.6674932440121969,
          0.6674932440121969,
          0.6674932440121969,
          0.6674932440121969,
          0.6674932440121969,
          0.6674932440121969,
          0.6749191946453519,
          0.6749191946453519,
          0.6749191946453519,
          0.6749191946453519,
          0.6749191946453519,
          0.6749191946453519,
          0.6749191946453519,
          0.6749191946453519,
          0.6749191946453519,
          0.6749191946453519,
          0.6749191946453519,
          0.6749191946453519,
          0.6749191946453519,
          0.6749191946453519,
          0.6749191946453519,
          0.6749191946453519,
          0.6749191946453519,
          0.6749191946453519,
          0.7832124961747063,
          0.7832124961747063,
          0.7832124961747063,
          0.7832124961747063,
          0.7832124961747063,
          0.7832124961747063,
          0.7832124961747063,
          0.7832124961747063,
          0.7832124961747063,
          0.7832124961747063,
          0.7832124961747063,
          0.7832124961747063,
          0.7832124961747063,
          0.7832124961747063,
          0.7832124961747063,
          0.7832124961747063,
          0.7832124961747063,
          0.7832124961747063,
          0.6418214109208848,
          0.6418214109208848,
          0.6418214109208848,
          0.6418214109208848,
          0.6418214109208848,
          0.6418214109208848,
          0.6418214109208848,
          0.6418214109208848,
          0.6418214109208848,
          0.6418214109208848,
          0.6418214109208848,
          0.6418214109208848,
          0.6418214109208848,
          0.6418214109208848,
          0.6418214109208848,
          0.6418214109208848,
          0.6418214109208848,
          0.6418214109208848,
          0.648163186179267,
          0.648163186179267,
          0.648163186179267,
          0.648163186179267,
          0.648163186179267,
          0.648163186179267,
          0.648163186179267,
          0.648163186179267,
          0.648163186179267,
          0.648163186179267,
          0.648163186179267,
          0.648163186179267,
          0.648163186179267,
          0.648163186179267,
          0.648163186179267,
          0.648163186179267,
          0.648163186179267,
          0.648163186179267,
          0.6712625556521945,
          0.6712625556521945,
          0.6712625556521945,
          0.6712625556521945,
          0.6712625556521945,
          0.6712625556521945,
          0.6712625556521945,
          0.6712625556521945,
          0.6712625556521945,
          0.6712625556521945,
          0.6712625556521945,
          0.6712625556521945,
          0.6712625556521945,
          0.6712625556521945,
          0.6712625556521945,
          0.6712625556521945,
          0.6712625556521945,
          0.6712625556521945,
          0.8038990828726027,
          0.8038990828726027,
          0.8038990828726027,
          0.8038990828726027,
          0.8038990828726027,
          0.8038990828726027,
          0.8038990828726027,
          0.8038990828726027,
          0.8038990828726027,
          0.8038990828726027,
          0.8038990828726027,
          0.8038990828726027,
          0.8038990828726027,
          0.8038990828726027,
          0.8038990828726027,
          0.8038990828726027,
          0.8038990828726027,
          0.8038990828726027,
          0.6435654958089193,
          0.6435654958089193,
          0.6435654958089193,
          0.6435654958089193,
          0.6435654958089193,
          0.6435654958089193,
          0.6435654958089193,
          0.6435654958089193,
          0.6435654958089193,
          0.6435654958089193,
          0.6435654958089193,
          0.6435654958089193,
          0.6435654958089193,
          0.6435654958089193,
          0.6435654958089193,
          0.6435654958089193,
          0.6435654958089193,
          0.6435654958089193,
          0.653283695379893,
          0.653283695379893,
          0.653283695379893,
          0.653283695379893,
          0.653283695379893,
          0.653283695379893,
          0.653283695379893,
          0.653283695379893,
          0.653283695379893,
          0.653283695379893,
          0.653283695379893,
          0.653283695379893,
          0.653283695379893,
          0.653283695379893,
          0.653283695379893,
          0.653283695379893,
          0.653283695379893,
          0.653283695379893
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "height": 400,
        "legend": {
         "title": {
          "text": "phase"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "shapes": [
         {
          "line": {
           "color": "green",
           "dash": "dash",
           "width": 3
          },
          "type": "line",
          "x0": 5,
          "x1": 5,
          "xref": "x",
          "y0": 0,
          "y1": 1,
          "yref": "y domain"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Loss vs epochs - GNN training"
        },
        "width": 700,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch #"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.line(\n",
    "    output_train,\n",
    "    x='epoch',\n",
    "    y='loss',\n",
    "    color='phase',\n",
    "    markers=True)\n",
    "\n",
    "fig.add_vline(x=trainer.epoch_saved_model, line_width=3, line_dash=\"dash\", line_color=\"green\")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Epoch #',\n",
    "    yaxis_title='Loss',\n",
    "    title='Loss vs epochs - GNN training',\n",
    "    width=700, height=400,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's print a few metrics of interest for classification tasks: the Area under the ROC curve (AUC), and for a threshold of 0.5 the precision, the recall, the accuracy and the f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for training:\n",
      "AUC: 0.6\n",
      "Considering a threshold of 0.5\n",
      "- Precision: 0.6\n",
      "- Recall: 0.6\n",
      "- Accuracy: 0.6\n",
      "- F1: 0.6\n",
      "\n",
      "Metrics for validation:\n",
      "AUC: 0.8\n",
      "Considering a threshold of 0.5\n",
      "- Precision: 1.0\n",
      "- Recall: 0.4\n",
      "- Accuracy: 0.7\n",
      "- F1: 0.6\n",
      "\n",
      "Metrics for testing:\n",
      "AUC: 0.4\n",
      "Considering a threshold of 0.5\n",
      "- Precision: 0.0\n",
      "- Recall: 0.0\n",
      "- Accuracy: 0.5\n",
      "- F1: 0.0\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "df = pd.concat([output_train, output_test])\n",
    "df_plot = df[(df.epoch == trainer.epoch_saved_model) | ((df.epoch == trainer.epoch_saved_model) & (df.phase == 'testing'))]\n",
    "\n",
    "for idx, set in enumerate(['training', 'validation', 'testing']):\n",
    "    df_plot_phase = df_plot[(df_plot.phase == set)]\n",
    "    y_true = df_plot_phase.target\n",
    "    y_score = np.array(df_plot_phase.output.values.tolist())[:, 1]\n",
    "\n",
    "    print(f'\\nMetrics for {set}:')\n",
    "    fpr_roc, tpr_roc, thr_roc = roc_curve(y_true, y_score)\n",
    "    auc_score = auc(fpr_roc, tpr_roc)\n",
    "    print(f'AUC: {round(auc_score, 1)}')\n",
    "    print(f'Considering a threshold of {threshold}')\n",
    "    y_pred = (y_score > threshold)*1\n",
    "    print(f'- Precision: {round(precision_score(y_true, y_pred), 1)}')\n",
    "    print(f'- Recall: {round(recall_score(y_true, y_pred), 1)}')\n",
    "    print(f'- Accuracy: {round(accuracy_score(y_true, y_pred), 1)}')\n",
    "    print(f'- F1: {round(f1_score(y_true, y_pred), 1)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course such metrics results are not satisfying from the performance point of view, but we did use only 100 data points, which of course is not enough to train a neural network. Now let's do the same exercise but using grids instead of graphs and CNNs instead of GNNs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training CNNs the user can create `GridDataset` instances.\n",
    "\n",
    "A few notes about `GridDataset` parameters:\n",
    "- By default, all features contained in the HDF5 files are used, but the user can specify `features` in `GridDataset` if not all of them are needed. Since grids features are derived from node and edge features mapped from graphs to grid, the easiest way to see which features are available is to look at the HDF5 file, as explained in detail in `data_generation_ppi.ipynb`, section \"Other tools\".  \n",
    "- As for graphs, if we want to perform regression, `task` can be assigned to `regress` and the `target` to `BA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.dataset:\n",
      "Checking dataset Integrity...\n",
      "INFO:deeprankcore.dataset:Target classes set up to: [0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "   ['data/ppi/processed/residue/proc-28835.hdf5', 'data/ppi/processed/residue/proc-28842.hdf5', 'data/ppi/processed/residue/proc-28839.hdf5', 'data/ppi/processed/residue/proc-28838.hdf5', 'data/ppi/processed/residue/proc-28843.hdf5', 'data/ppi/processed/residue/proc-28834.hdf5', 'data/ppi/processed/residue/proc-28837.hdf5', 'data/ppi/processed/residue/proc-28840.hdf5', 'data/ppi/processed/residue/proc-28841.hdf5', 'data/ppi/processed/residue/proc-28836.hdf5'] dataset                 : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 449.68it/s, entry_name=proc-28836.hdf5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.dataset:\n",
      "Checking dataset Integrity...\n",
      "INFO:deeprankcore.dataset:Target classes set up to: [0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading validation data...\n",
      "   ['data/ppi/processed/residue/proc-28835.hdf5', 'data/ppi/processed/residue/proc-28842.hdf5', 'data/ppi/processed/residue/proc-28839.hdf5', 'data/ppi/processed/residue/proc-28838.hdf5', 'data/ppi/processed/residue/proc-28843.hdf5', 'data/ppi/processed/residue/proc-28834.hdf5', 'data/ppi/processed/residue/proc-28837.hdf5', 'data/ppi/processed/residue/proc-28840.hdf5', 'data/ppi/processed/residue/proc-28841.hdf5', 'data/ppi/processed/residue/proc-28836.hdf5'] dataset                 : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 222.46it/s, entry_name=proc-28836.hdf5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.dataset:\n",
      "Checking dataset Integrity...\n",
      "INFO:deeprankcore.dataset:Target classes set up to: [0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading test data...\n",
      "   ['data/ppi/processed/residue/proc-28835.hdf5', 'data/ppi/processed/residue/proc-28842.hdf5', 'data/ppi/processed/residue/proc-28839.hdf5', 'data/ppi/processed/residue/proc-28838.hdf5', 'data/ppi/processed/residue/proc-28843.hdf5', 'data/ppi/processed/residue/proc-28834.hdf5', 'data/ppi/processed/residue/proc-28837.hdf5', 'data/ppi/processed/residue/proc-28840.hdf5', 'data/ppi/processed/residue/proc-28841.hdf5', 'data/ppi/processed/residue/proc-28836.hdf5'] dataset                 : 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:00<00:00, 894.69it/s, entry_name=proc-28836.hdf5]\n"
     ]
    }
   ],
   "source": [
    "target = \"binary\"\n",
    "task = \"classif\"\n",
    "\n",
    "print('Loading training data...')\n",
    "dataset_train = GridDataset(\n",
    "    hdf5_path = input_data_path,\n",
    "    subset = list(df_train.entry), # selects only data points with ids in df_train.entry\n",
    "    target = target,\n",
    "    task = task\n",
    ")\n",
    "print('\\nLoading validation data...')\n",
    "dataset_val = GridDataset(\n",
    "    hdf5_path = input_data_path,\n",
    "    subset = list(df_valid.entry), # selects only data points with ids in df_valid.entry\n",
    "    target = target,\n",
    "    task = task\n",
    ")\n",
    "print('\\nLoading test data...')\n",
    "dataset_test = GridDataset(\n",
    "    hdf5_path = input_data_path,\n",
    "    subset = list(df_test.entry), # selects only data points with ids in df_test.entry\n",
    "    target = target,\n",
    "    task = task\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer\n",
    "\n",
    "As for graphs, the class `Trainer` is used for training, validation and testing of the PyTorch-based CNN. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Also in this case, `neuralnet` can be whatever neural network class which inherits from `torch.nn.Module`, and it shouldn't be specific to regression or classification in terms of output shape. Here we use `CnnClassification` (in `deeprankcore.neuralnets.cnn.model3d`), whose architecture is shown below:\n",
    "  ```python\n",
    "    class CnnClassification(torch.nn.Module):\n",
    "\n",
    "        def __init__(self, num_features, box_shape):\n",
    "            super().__init__()\n",
    "\n",
    "            self.convlayer_000 = torch.nn.Conv3d(num_features, 4, kernel_size=2)\n",
    "            self.convlayer_001 = torch.nn.MaxPool3d((2, 2, 2))\n",
    "            self.convlayer_002 = torch.nn.Conv3d(4, 5, kernel_size=2)\n",
    "            self.convlayer_003 = torch.nn.MaxPool3d((2, 2, 2))\n",
    "\n",
    "            size = self._get_conv_output(num_features, box_shape)\n",
    "\n",
    "            self.fclayer_000 = torch.nn.Linear(size, 84)\n",
    "            self.fclayer_001 = torch.nn.Linear(84, 2)\n",
    "\n",
    "        def _get_conv_output(self, num_features, shape):\n",
    "            inp = Variable(torch.rand(1, num_features, *shape))\n",
    "            out = self._forward_features(inp)\n",
    "            return out.data.view(1, -1).size(1)\n",
    "\n",
    "        def _forward_features(self, x):\n",
    "            x = F.relu(self.convlayer_000(x))\n",
    "            x = self.convlayer_001(x)\n",
    "            x = F.relu(self.convlayer_002(x))\n",
    "            x = self.convlayer_003(x)\n",
    "            return x\n",
    "\n",
    "        def forward(self, data):\n",
    "            x = self._forward_features(data.x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = F.relu(self.fclayer_000(x))\n",
    "            x = self.fclayer_001(x)\n",
    "            return x\n",
    "  ```\n",
    "- The rest of the `Trainer` parameters can be used as explained already for graphs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.trainer:Device set to cpu.\n",
      "INFO:deeprankcore.trainer:No loss function provided, the default loss function for classif tasks is used: <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "INFO:deeprankcore.trainer:Training set loaded\n",
      "\n",
      "INFO:deeprankcore.trainer:Validation set loaded\n",
      "\n",
      "INFO:deeprankcore.trainer:Epoch 0:\n",
      "INFO:deeprankcore.trainer:training loss 0.697644485367669 | time 4.762144088745117\n",
      "INFO:deeprankcore.trainer:validation loss 0.6870552036497328 | time 1.185007095336914\n",
      "INFO:deeprankcore.trainer:Epoch 1:\n",
      "INFO:deeprankcore.trainer:training loss 0.7181484434339735 | time 5.222409009933472\n",
      "INFO:deeprankcore.trainer:validation loss 0.696343461672465 | time 1.183293104171753\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 1.\n",
      "INFO:deeprankcore.trainer:Epoch 2:\n",
      "INFO:deeprankcore.trainer:training loss 0.6928930679957072 | time 5.244858980178833\n",
      "INFO:deeprankcore.trainer:validation loss 0.7079185048739115 | time 1.1831212043762207\n",
      "INFO:deeprankcore.trainer:Validation loss did not decrease (0.696343 --> 0.707919). EarlyStopping counter: 1 out of 5\n",
      "INFO:deeprankcore.trainer:Epoch 3:\n",
      "INFO:deeprankcore.trainer:training loss 0.7005800737275018 | time 5.19649600982666\n",
      "INFO:deeprankcore.trainer:validation loss 0.6936482323540581 | time 1.1825008392333984\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 3.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.696343 --> 0.693648).\n",
      "INFO:deeprankcore.trainer:Epoch 4:\n",
      "INFO:deeprankcore.trainer:training loss 0.6961994369824728 | time 5.192233085632324\n",
      "INFO:deeprankcore.trainer:validation loss 0.695694367090861 | time 1.1766061782836914\n",
      "INFO:deeprankcore.trainer:Validation loss did not decrease (0.693648 --> 0.695694). EarlyStopping counter: 1 out of 5\n",
      "INFO:deeprankcore.trainer:Epoch 5:\n",
      "INFO:deeprankcore.trainer:training loss 0.6959073411093818 | time 5.206750154495239\n",
      "INFO:deeprankcore.trainer:validation loss 0.6955969863467746 | time 1.1841771602630615\n",
      "INFO:deeprankcore.trainer:Validation loss did not decrease (0.693648 --> 0.695597). EarlyStopping counter: 2 out of 5\n",
      "INFO:deeprankcore.trainer:Epoch 6:\n",
      "INFO:deeprankcore.trainer:training loss 0.695786284075843 | time 5.5117340087890625\n",
      "INFO:deeprankcore.trainer:validation loss 0.695502758026123 | time 1.209658145904541\n",
      "INFO:deeprankcore.trainer:Validation loss did not decrease (0.693648 --> 0.695503). EarlyStopping counter: 3 out of 5\n",
      "INFO:deeprankcore.trainer:Epoch 7:\n",
      "INFO:deeprankcore.trainer:training loss 0.6956880291302999 | time 5.544136047363281\n",
      "INFO:deeprankcore.trainer:validation loss 0.6954099469714694 | time 1.2203071117401123\n",
      "INFO:deeprankcore.trainer:Validation loss did not decrease (0.693648 --> 0.695410). EarlyStopping counter: 4 out of 5\n",
      "INFO:deeprankcore.trainer:Epoch 8:\n",
      "INFO:deeprankcore.trainer:training loss 0.6955645945337083 | time 5.38475489616394\n",
      "INFO:deeprankcore.trainer:validation loss 0.6953213214874268 | time 1.22133207321167\n",
      "INFO:deeprankcore.trainer:Validation loss did not decrease (0.693648 --> 0.695321). EarlyStopping counter: 5 out of 5\n",
      "INFO:deeprankcore.trainer:EarlyStopping activated at epoch # 8 because patience of 5 has been reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at epoch 3\n",
      "Total # of parameters: 122599\n",
      "Total # of trainable parameters: 122599\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD\n",
    "lr = 1e-3\n",
    "weight_decay = 0.001\n",
    "epochs = 20\n",
    "batch_size = 8\n",
    "earlystop_patience = 5\n",
    "earlystop_maxgap = 0.1\n",
    "min_epoch = 10\n",
    "\n",
    "trainer = Trainer(\n",
    "    neuralnet = CnnClassification,\n",
    "    dataset_train = dataset_train,\n",
    "    dataset_val = dataset_val,\n",
    "    dataset_test = dataset_test,\n",
    "    output_exporters = [HDF5OutputExporter(os.path.join(output_path, \"cnn_classif\"))]\n",
    ")\n",
    "\n",
    "trainer.configure_optimizers(optimizer, lr, weight_decay)\n",
    "\n",
    "trainer.train(\n",
    "    nepoch = epochs,\n",
    "    batch_size = batch_size,\n",
    "    earlystop_patience = earlystop_patience,\n",
    "    earlystop_maxgap = earlystop_maxgap,\n",
    "    min_epoch = min_epoch,\n",
    "    validate = True,\n",
    "    filename = os.path.join(output_path, \"cnn_classif\", \"model.pth.tar\"))\n",
    "\n",
    "epoch = trainer.epoch_saved_model\n",
    "print(f\"Model saved at epoch {epoch}\")\n",
    "pytorch_total_params = sum(p.numel() for p in trainer.model.parameters())\n",
    "print(f'Total # of parameters: {pytorch_total_params}')\n",
    "pytorch_trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
    "print(f'Total # of trainable parameters: {pytorch_trainable_params}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing\n",
    "\n",
    "And we can test the trained model on our `dataset_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.trainer:Loading independent testing dataset...\n",
      "INFO:deeprankcore.trainer:Testing set loaded\n",
      "\n",
      "INFO:deeprankcore.trainer:testing loss 0.6965798735618591 | time 0.6703002452850342\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results visualization\n",
    "\n",
    "As for the GNN, we can finally inspect the results saved by `HDF5OutputExporter`, which can be found in `data/ppi/cnn_classif/` folder in the form of an HDF5 file, `output_exporter.hdf5`, together with the saved pre-trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase</th>\n",
       "      <th>epoch</th>\n",
       "      <th>entry</th>\n",
       "      <th>output</th>\n",
       "      <th>target</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-107135</td>\n",
       "      <td>[0.45900216698646545, 0.5409978032112122]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.697644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-128016</td>\n",
       "      <td>[0.4739755690097809, 0.5260244607925415]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.697644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-120135</td>\n",
       "      <td>[0.4273616671562195, 0.5726382732391357]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.697644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-119409</td>\n",
       "      <td>[0.46763601899147034, 0.5323639512062073]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.697644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-153952</td>\n",
       "      <td>[0.48576146364212036, 0.5142385363578796]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.697644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phase  epoch                      entry  \\\n",
       "0  training    0.0  residue-ppi:M-P:BA-107135   \n",
       "1  training    0.0  residue-ppi:M-P:BA-128016   \n",
       "2  training    0.0  residue-ppi:M-P:BA-120135   \n",
       "3  training    0.0  residue-ppi:M-P:BA-119409   \n",
       "4  training    0.0  residue-ppi:M-P:BA-153952   \n",
       "\n",
       "                                      output  target      loss  \n",
       "0  [0.45900216698646545, 0.5409978032112122]     0.0  0.697644  \n",
       "1   [0.4739755690097809, 0.5260244607925415]     0.0  0.697644  \n",
       "2   [0.4273616671562195, 0.5726382732391357]     1.0  0.697644  \n",
       "3  [0.46763601899147034, 0.5323639512062073]     0.0  0.697644  \n",
       "4  [0.48576146364212036, 0.5142385363578796]     0.0  0.697644  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_train = pd.read_hdf(os.path.join(output_path, \"cnn_classif\", \"output_exporter.hdf5\"), key=\"training\")\n",
    "output_test = pd.read_hdf(os.path.join(output_path, \"cnn_classif\", \"output_exporter.hdf5\"), key=\"testing\")\n",
    "output_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot also in this case the loss across the epochs for the training and the validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "phase=training<br>epoch=%{x}<br>loss=%{y}<extra></extra>",
         "legendgroup": "training",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "markers+lines",
         "name": "training",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8
         ],
         "xaxis": "x",
         "y": [
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.697644485367669,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.7181484434339735,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.6928930679957072,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.7005800737275018,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6961994369824728,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.6959073411093818,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.695786284075843,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6956880291302999,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083,
          0.6955645945337083
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "phase=validation<br>epoch=%{x}<br>loss=%{y}<extra></extra>",
         "legendgroup": "validation",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "markers+lines",
         "name": "validation",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8
         ],
         "xaxis": "x",
         "y": [
          0.6870552036497328,
          0.6870552036497328,
          0.6870552036497328,
          0.6870552036497328,
          0.6870552036497328,
          0.6870552036497328,
          0.6870552036497328,
          0.6870552036497328,
          0.6870552036497328,
          0.6870552036497328,
          0.6870552036497328,
          0.6870552036497328,
          0.6870552036497328,
          0.6870552036497328,
          0.6870552036497328,
          0.6870552036497328,
          0.6870552036497328,
          0.6870552036497328,
          0.696343461672465,
          0.696343461672465,
          0.696343461672465,
          0.696343461672465,
          0.696343461672465,
          0.696343461672465,
          0.696343461672465,
          0.696343461672465,
          0.696343461672465,
          0.696343461672465,
          0.696343461672465,
          0.696343461672465,
          0.696343461672465,
          0.696343461672465,
          0.696343461672465,
          0.696343461672465,
          0.696343461672465,
          0.696343461672465,
          0.7079185048739115,
          0.7079185048739115,
          0.7079185048739115,
          0.7079185048739115,
          0.7079185048739115,
          0.7079185048739115,
          0.7079185048739115,
          0.7079185048739115,
          0.7079185048739115,
          0.7079185048739115,
          0.7079185048739115,
          0.7079185048739115,
          0.7079185048739115,
          0.7079185048739115,
          0.7079185048739115,
          0.7079185048739115,
          0.7079185048739115,
          0.7079185048739115,
          0.6936482323540581,
          0.6936482323540581,
          0.6936482323540581,
          0.6936482323540581,
          0.6936482323540581,
          0.6936482323540581,
          0.6936482323540581,
          0.6936482323540581,
          0.6936482323540581,
          0.6936482323540581,
          0.6936482323540581,
          0.6936482323540581,
          0.6936482323540581,
          0.6936482323540581,
          0.6936482323540581,
          0.6936482323540581,
          0.6936482323540581,
          0.6936482323540581,
          0.695694367090861,
          0.695694367090861,
          0.695694367090861,
          0.695694367090861,
          0.695694367090861,
          0.695694367090861,
          0.695694367090861,
          0.695694367090861,
          0.695694367090861,
          0.695694367090861,
          0.695694367090861,
          0.695694367090861,
          0.695694367090861,
          0.695694367090861,
          0.695694367090861,
          0.695694367090861,
          0.695694367090861,
          0.695694367090861,
          0.6955969863467746,
          0.6955969863467746,
          0.6955969863467746,
          0.6955969863467746,
          0.6955969863467746,
          0.6955969863467746,
          0.6955969863467746,
          0.6955969863467746,
          0.6955969863467746,
          0.6955969863467746,
          0.6955969863467746,
          0.6955969863467746,
          0.6955969863467746,
          0.6955969863467746,
          0.6955969863467746,
          0.6955969863467746,
          0.6955969863467746,
          0.6955969863467746,
          0.695502758026123,
          0.695502758026123,
          0.695502758026123,
          0.695502758026123,
          0.695502758026123,
          0.695502758026123,
          0.695502758026123,
          0.695502758026123,
          0.695502758026123,
          0.695502758026123,
          0.695502758026123,
          0.695502758026123,
          0.695502758026123,
          0.695502758026123,
          0.695502758026123,
          0.695502758026123,
          0.695502758026123,
          0.695502758026123,
          0.6954099469714694,
          0.6954099469714694,
          0.6954099469714694,
          0.6954099469714694,
          0.6954099469714694,
          0.6954099469714694,
          0.6954099469714694,
          0.6954099469714694,
          0.6954099469714694,
          0.6954099469714694,
          0.6954099469714694,
          0.6954099469714694,
          0.6954099469714694,
          0.6954099469714694,
          0.6954099469714694,
          0.6954099469714694,
          0.6954099469714694,
          0.6954099469714694,
          0.6953213214874268,
          0.6953213214874268,
          0.6953213214874268,
          0.6953213214874268,
          0.6953213214874268,
          0.6953213214874268,
          0.6953213214874268,
          0.6953213214874268,
          0.6953213214874268,
          0.6953213214874268,
          0.6953213214874268,
          0.6953213214874268,
          0.6953213214874268,
          0.6953213214874268,
          0.6953213214874268,
          0.6953213214874268,
          0.6953213214874268,
          0.6953213214874268
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "height": 400,
        "legend": {
         "title": {
          "text": "phase"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "shapes": [
         {
          "line": {
           "color": "green",
           "dash": "dash",
           "width": 3
          },
          "type": "line",
          "x0": 3,
          "x1": 3,
          "xref": "x",
          "y0": 0,
          "y1": 1,
          "yref": "y domain"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Loss vs epochs - CNN training"
        },
        "width": 700,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch #"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.line(\n",
    "    output_train,\n",
    "    x='epoch',\n",
    "    y='loss',\n",
    "    color='phase',\n",
    "    markers=True)\n",
    "\n",
    "fig.add_vline(x=trainer.epoch_saved_model, line_width=3, line_dash=\"dash\", line_color=\"green\")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Epoch #',\n",
    "    yaxis_title='Loss',\n",
    "    title='Loss vs epochs - CNN training',\n",
    "    width=700, height=400,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some metrics of interest for classification tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for training:\n",
      "AUC: 0.5\n",
      "Considering a threshold of 0.5\n",
      "- Precision: 0.6\n",
      "- Recall: 0.2\n",
      "- Accuracy: 0.5\n",
      "- F1: 0.4\n",
      "\n",
      "Metrics for validation:\n",
      "AUC: 0.6\n",
      "Considering a threshold of 0.5\n",
      "- Precision: 0.5\n",
      "- Recall: 1.0\n",
      "- Accuracy: 0.5\n",
      "- F1: 0.7\n",
      "\n",
      "Metrics for testing:\n",
      "AUC: 0.4\n",
      "Considering a threshold of 0.5\n",
      "- Precision: 0.5\n",
      "- Recall: 1.0\n",
      "- Accuracy: 0.5\n",
      "- F1: 0.7\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "df = pd.concat([output_train, output_test])\n",
    "df_plot = df[(df.epoch == trainer.epoch_saved_model) | ((df.epoch == trainer.epoch_saved_model) & (df.phase == 'testing'))]\n",
    "\n",
    "for idx, set in enumerate(['training', 'validation', 'testing']):\n",
    "    df_plot_phase = df_plot[(df_plot.phase == set)]\n",
    "    y_true = df_plot_phase.target\n",
    "    y_score = np.array(df_plot_phase.output.values.tolist())[:, 1]\n",
    "\n",
    "    print(f'\\nMetrics for {set}:')\n",
    "    fpr_roc, tpr_roc, thr_roc = roc_curve(y_true, y_score)\n",
    "    auc_score = auc(fpr_roc, tpr_roc)\n",
    "    print(f'AUC: {round(auc_score, 1)}')\n",
    "    print(f'Considering a threshold of {threshold}')\n",
    "    y_pred = (y_score > threshold)*1\n",
    "    print(f'- Precision: {round(precision_score(y_true, y_pred), 1)}')\n",
    "    print(f'- Recall: {round(recall_score(y_true, y_pred), 1)}')\n",
    "    print(f'- Accuracy: {round(accuracy_score(y_true, y_pred), 1)}')\n",
    "    print(f'- F1: {round(f1_score(y_true, y_pred), 1)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results appear to be less favorable for GNN compared to CNN, but it's important to note that the dataset used in this analysis is not sufficiently large to provide conclusive and reliable insights. Depending on your specific application, you might find regression, classification, GNNs, and/or CNNs to be valuable options. Feel free to choose the approach that best aligns with your particular problem!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprankcore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
