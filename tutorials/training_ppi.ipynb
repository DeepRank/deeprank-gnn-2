{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepRank-Core basic Protein-Protein Interface training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example data used in this tutorial are available on [tdb, zenodo?](). To download the data, please [visit the dataset page for this workshop and click the “Download all” button. Unzip the downloaded file, and] save the contents as a folder called `data/` in the same directory of this notebook. (The name data and the folder location are optional but recommended, as this are the name and the location we will use to refer to the folder throughout the tutorial.)\n",
    "\n",
    "The dataset contains only 100 data points, which are obviously not enough to develop an impactful predictive model, and the scope of its use is indeed only demonstrative and informative for the users."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Follow the [updated instructions](https://github.com/DeepRank/deeprank-core#installation) in the README.md on the main branch for successfully installing `deeprankcore` package.\n",
    "2. To test the environment in which` deeprankcore` has been (successfully) installed, first clone [deeprank-core repository](https://github.com/DeepRank/deeprank-core). Navigate into it, and after having activated the environment and installed [pytest](https://anaconda.org/anaconda/pytest), run `pytest tests`. All tests should pass. We recommend installing `deeprankcore` and all its dependencies into a [conda](https://docs.conda.io/en/latest/) environment.\n",
    "3. Additionally, for this tutorial you need to install [scikit-learn](https://scikit-learn.org/stable/install.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"margin-left: 1.5rem\" align=\"right\" src=\"images/training_ppi.png\" width=\"400\">\n",
    "\n",
    "This tutorial will demonstrate the use of DeepRank-Core for training Graph Neural Networks (GNNs) and Convolutional Neural Networks (CNNs) using Protein-Protein Interface (PPI) data for classification and regression predictive tasks.\n",
    "\n",
    "This tutorial assumes that the PPI data of interest have already been generated and saved into [HDF5 files](https://en.wikipedia.org/wiki/Hierarchical_Data_Format), with the data structure that DeepRank-Core expects. Such data have already been generated in `data/ppi/processed/` folder for the user, but for more details about the process please refer to the tutorial notebook [data_generation_ppi.ipynb](insert_link). It contains a detailed description of how such data are generated starting from the PDB files and gives instructions for generating different resolutions' data, i.e. residue- and atomic-level.\n",
    "\n",
    "This tutorial assumes also a basic knowledge of the [PyTorch](https://pytorch.org/) framework, on top of which the machine learning pipeline of DeepRank-Core has been developed. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"margin-right: 1.5rem\" align=\"left\" src=\"images/pmhc_pdb_example.png\" width=\"200\"/>\n",
    "\n",
    "The example dataset that we provide contains PDB files representing [Major Histocompatibility Complex (MHC) protein](https://en.wikipedia.org/wiki/Major_histocompatibility_complex) + peptide (pMHC) complexes, which play a key role in T-cell immunity. We are interested in predicting the Binding Affinity (BA) of the complexes, which can be used to determine the most suitable mutated tumor peptides as vaccine candidates.\n",
    "\n",
    "PDB models used in this tutorial have been generated with [PANDORA](https://github.com/X-lab-3D/PANDORA), an anchor restrained modeling pipeline for generating peptide-MHC structures. While target data, so the BA values for such pMHC complexes, have been retrieved from [MHCFlurry 2.0](https://data.mendeley.com/datasets/zx3kjzc3yx).\n",
    "\n",
    "On the left an example of a pMHC structure is shown, rendered using [ProteinViewer](https://marketplace.visualstudio.com/items?itemName=ArianJamasb.protein-viewer). The MHC protein is displayed in green, while the peptide is in orange."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the libraries needed for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import numpy as np\n",
    "np.seterr(divide = 'ignore')\n",
    "np.seterr(invalid='ignore')\n",
    "from deeprankcore.dataset import GraphDataset\n",
    "from deeprankcore.trainer import Trainer\n",
    "from deeprankcore.neuralnets.gnn.naive_gnn import NaiveNetwork\n",
    "from deeprankcore.utils.exporters import HDF5OutputExporter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths and sets\n",
    "\n",
    "Let's define the paths for reading the processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"residue\"\n",
    "processed_data_path = os.path.join(\"data\", \"ppi\", \"processed\", level)\n",
    "input_data_path = glob.glob(os.path.join(processed_data_path, '*.hdf5'))\n",
    "output_path = os.path.join(\"data\", \"ppi\") # for saving predictions results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The levels refer to a different molecular resolution, either residue- or atomic-level. In residue-level graphs, each node represents one amino acid residue, while in atomic-level graphs each node represents one atom within the amino acid residues. In this tutorial, we will use residue-level data, but the same code could be applied to atomic-level data with no changes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's definea Pandas dataframe containing data points' IDs and the binary target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>residue-ppi:M-P:BA-102611</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>residue-ppi:M-P:BA-102669</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>residue-ppi:M-P:BA-102719</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>residue-ppi:M-P:BA-114468</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>residue-ppi:M-P:BA-115138</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       entry  target\n",
       "0  residue-ppi:M-P:BA-102611     1.0\n",
       "1  residue-ppi:M-P:BA-102669     0.0\n",
       "2  residue-ppi:M-P:BA-102719     0.0\n",
       "3  residue-ppi:M-P:BA-114468     0.0\n",
       "4  residue-ppi:M-P:BA-115138     0.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict = {}\n",
    "df_dict['entry'] = []\n",
    "df_dict['target'] = []\n",
    "for fname in input_data_path:\n",
    "    with h5py.File(fname, 'r') as hdf5:\n",
    "        for mol in hdf5.keys():\n",
    "            target_value = float(hdf5[mol][\"target_values\"][\"binary\"][()])\n",
    "            df_dict['entry'].append(mol)\n",
    "            df_dict['target'].append(target_value)\n",
    "\n",
    "df = pd.DataFrame(data=df_dict)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in [data_generation_ppi.ipynb tutorial](insert_link), for each data point there are two targets: \"BA\" and \"binary\". The first represents the continuous Binding Affinity value of the complex, while the second represents its binary representation, being 0 (BA > 500 nM) a not-binding complex and 1 (BA <= 500 nM) binding one.\n",
    "\n",
    "The dataframe `df` is used only to split data points into training, validation and test sets according to the \"binary\" target - using the target stratification, to keep the proportion of 0s and 1s sort of constant among the different sets. Training and validation set will be used during the training for updating the netowork's weights, while the test set will be held out as an indipendent test and will be used for later model's evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data statistics:\n",
      "\n",
      "Total samples: 100\n",
      "\n",
      "Training set: 72 samples, 72%\n",
      "\t- Class 0: 36 samples, 50%\n",
      "\t- Class 1: 36 samples, 50%\n",
      "Validation set: 18 samples, 18%\n",
      "\t- Class 0: 9 samples, 50%\n",
      "\t- Class 1: 9 samples, 50%\n",
      "Testing set: 10 samples, 10%\n",
      "\t- Class 0: 5 samples, 50%\n",
      "\t- Class 1: 5 samples, 50%\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.1, stratify=df.target, random_state=42)\n",
    "df_train, df_valid = train_test_split(df_train, test_size=0.2, stratify=df_train.target, random_state=42)\n",
    "\n",
    "print(f'Data statistics:\\n')\n",
    "print(f'Total samples: {len(df)}\\n')\n",
    "print(f'Training set: {len(df_train)} samples, {round(100*len(df_train)/len(df))}%')\n",
    "print(f'\\t- Class 0: {len(df_train[df_train.target == 0])} samples, {round(100*len(df_train[df_train.target == 0])/len(df_train))}%')\n",
    "print(f'\\t- Class 1: {len(df_train[df_train.target == 1])} samples, {round(100*len(df_train[df_train.target == 1])/len(df_train))}%')\n",
    "print(f'Validation set: {len(df_valid)} samples, {round(100*len(df_valid)/len(df))}%')\n",
    "print(f'\\t- Class 0: {len(df_valid[df_valid.target == 0])} samples, {round(100*len(df_valid[df_valid.target == 0])/len(df_valid))}%')\n",
    "print(f'\\t- Class 1: {len(df_valid[df_valid.target == 1])} samples, {round(100*len(df_valid[df_valid.target == 1])/len(df_valid))}%')\n",
    "print(f'Testing set: {len(df_test)} samples, {round(100*len(df_test)/len(df))}%')\n",
    "print(f'\\t- Class 0: {len(df_test[df_test.target == 0])} samples, {round(100*len(df_test[df_test.target == 0])/len(df_test))}%')\n",
    "print(f'\\t- Class 1: {len(df_test[df_test.target == 1])} samples, {round(100*len(df_test[df_test.target == 1])/len(df_test))}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Let's train a GNN and a CNN for a classification predictive task, which consists in predicting the \"binary\" target values. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GraphDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training GNNs the user can create `GraphDataset` instances. The latter class inherits from `DeeprankDataset` class, which in turns inherits from `Dataset` [PyTorch geometric class](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/data/dataset.html), a base class for creating graph datasets.\n",
    "\n",
    "A few notes about `GraphDataset` parameters:\n",
    "- By default, all features contained in the HDF5 files are used, but the user can specify `node_features` and `edge_features` in `GraphDataset` if not all of them are needed. See the [docs](https://deeprankcore.readthedocs.io/en/latest/features.html) for more details about all the possible pre-implemented features. \n",
    "- For the `GraphDataset` class only it is possible to define a dictionary to indicate which transformations to apply to the features, being the transformations lambda functions and/or standardization. If `True`, standardization is applied after transformation, if the latter is present. Standardization consists in applying the following formula on each feature's value: ${x' = \\frac{x - \\mu}{\\sigma}}$, being ${\\mu}$ the mean and ${\\sigma}$ the standard deviation. Standardization is a scaling method where the values are centered around mean with a unit standard deviation. In the example below we will apply a logarithmic transformation and then standardization to all the features. It is also possible to use specific features as keys for indicating that we want to apply transformation and/or standardization to few features only. \n",
    "- Since we are applying standardization, we need to use training features' means and standard deviations to scale validation and test sets. For doing so, `train` and `dataset_train` parameters are used. When `train` is set `False`, a `dataset_train` of the same class must be provided and it will be used to scale the validation/testing set according to its features values. You need to pass `features_transform` to the training dataset only, since in other cases it will be ignored and only the one of `dataset_train` will be considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "   ['data/ppi/processed/residue/proc-28835.hdf5', 'data/ppi/processed/residue/proc-28842.hdf5', 'data/ppi/processed/residue/proc-28839.hdf5', 'data/ppi/processed/residue/proc-28838.hdf5', 'data/ppi/processed/residue/proc-28843.hdf5', 'data/ppi/processed/residue/proc-28834.hdf5', 'data/ppi/processed/residue/proc-28837.hdf5', 'data/ppi/processed/residue/proc-28840.hdf5', 'data/ppi/processed/residue/proc-28841.hdf5', 'data/ppi/processed/residue/proc-28836.hdf5'] dataset                 : 100%|██████████| 10/10 [00:00<00:00, 383.71it/s, entry_name=proc-28836.hdf5]\n",
      "Loading validation data...\n",
      "   ['data/ppi/processed/residue/proc-28835.hdf5', 'data/ppi/processed/residue/proc-28842.hdf5', 'data/ppi/processed/residue/proc-28839.hdf5', 'data/ppi/processed/residue/proc-28838.hdf5', 'data/ppi/processed/residue/proc-28843.hdf5', 'data/ppi/processed/residue/proc-28834.hdf5', 'data/ppi/processed/residue/proc-28837.hdf5', 'data/ppi/processed/residue/proc-28840.hdf5', 'data/ppi/processed/residue/proc-28841.hdf5', 'data/ppi/processed/residue/proc-28836.hdf5'] dataset                 : 100%|██████████| 10/10 [00:00<00:00, 841.94it/s, entry_name=proc-28836.hdf5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "features_transform parameter, if set, will be ignored.\n",
      "features_transform will remain the same as the one used in training phase.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading test data...\n",
      "   ['data/ppi/processed/residue/proc-28835.hdf5', 'data/ppi/processed/residue/proc-28842.hdf5', 'data/ppi/processed/residue/proc-28839.hdf5', 'data/ppi/processed/residue/proc-28838.hdf5', 'data/ppi/processed/residue/proc-28843.hdf5', 'data/ppi/processed/residue/proc-28834.hdf5', 'data/ppi/processed/residue/proc-28837.hdf5', 'data/ppi/processed/residue/proc-28840.hdf5', 'data/ppi/processed/residue/proc-28841.hdf5', 'data/ppi/processed/residue/proc-28836.hdf5'] dataset                 : 100%|██████████| 10/10 [00:00<00:00, 851.07it/s, entry_name=proc-28836.hdf5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "features_transform parameter, if set, will be ignored.\n",
      "features_transform will remain the same as the one used in training phase.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target = \"binary\"\n",
    "task = \"classif\"\n",
    "features_transform = {'all': {'transform': lambda x: np.sqrt(x), 'standardize': True}}\n",
    "\n",
    "print('Loading training data...')\n",
    "dataset_train = GraphDataset(\n",
    "    hdf5_path = input_data_path,\n",
    "    subset = list(df_train.entry), # selects only data points with ids in df_train.entry\n",
    "    target = target,\n",
    "    task = task,\n",
    "    features_transform = features_transform\n",
    ")\n",
    "print('Loading validation data...')\n",
    "dataset_val = GraphDataset(\n",
    "    hdf5_path = input_data_path,\n",
    "    subset = list(df_valid.entry), # selects only data points with ids in df_valid.entry\n",
    "    target = target,\n",
    "    task = task,\n",
    "    train = False,\n",
    "    dataset_train = dataset_train\n",
    ")\n",
    "print('Loading test data...')\n",
    "dataset_test = GraphDataset(\n",
    "    hdf5_path = input_data_path,\n",
    "    subset = list(df_test.entry), # selects only data points with ids in df_test.entry\n",
    "    target = target,\n",
    "    task = task,\n",
    "    train = False,\n",
    "    dataset_train = dataset_train\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer\n",
    "\n",
    "The class `Trainer` implements training, validation and testing of PyTorch-based neural networks. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes about `Trainer` parameters:\n",
    "- `neuralnet` can be whatever neural network class which inherits from `torch.nn.Module`, and it shouldn't be specific to regression or classification in terms of output shape. `Trainer` class takes care of formatting the output shape according to the task. In this tutorial we will use `NaiveNetwork` (in `deeprankcore.neuralnets.gnn.naive_gnn`), whose architecture is shown below:\n",
    "  ```python\n",
    "  class NaiveConvolutionalLayer(Module):\n",
    "\n",
    "    def __init__(self, count_node_features, count_edge_features):\n",
    "        super().__init__()\n",
    "        message_size = 32\n",
    "        edge_input_size = 2 * count_node_features + count_edge_features\n",
    "        self._edge_mlp = Sequential(Linear(edge_input_size, message_size), ReLU())\n",
    "        node_input_size = count_node_features + message_size\n",
    "        self._node_mlp = Sequential(Linear(node_input_size, count_node_features), ReLU())\n",
    "\n",
    "    def forward(self, node_features, edge_node_indices, edge_features):\n",
    "        # generate messages over edges\n",
    "        node0_indices, node1_indices = edge_node_indices\n",
    "        node0_features = node_features[node0_indices]\n",
    "        node1_features = node_features[node1_indices]\n",
    "        message_input = torch.cat([node0_features, node1_features, edge_features], dim=1)\n",
    "        messages_per_neighbour = self._edge_mlp(message_input)\n",
    "        # aggregate messages\n",
    "        out = torch.zeros(node_features.shape[0], messages_per_neighbour.shape[1]).to(node_features.device)\n",
    "        message_sums_per_node = scatter_sum(messages_per_neighbour, node0_indices, dim=0, out=out)\n",
    "        # update nodes\n",
    "        node_input = torch.cat([node_features, message_sums_per_node], dim=1)\n",
    "        node_output = self._node_mlp(node_input)\n",
    "        return node_output\n",
    "\n",
    "    class NaiveNetwork(Module):\n",
    "\n",
    "        def __init__(self, input_shape: int, output_shape: int, input_shape_edge: int):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                input_shape (int): Number of node input features.\n",
    "                output_shape (int): Number of output value per graph.\n",
    "                input_shape_edge (int): Number of edge input features.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self._external1 = NaiveConvolutionalLayer(input_shape, input_shape_edge)\n",
    "            self._external2 = NaiveConvolutionalLayer(input_shape, input_shape_edge)\n",
    "            hidden_size = 128\n",
    "            self._graph_mlp = Sequential(Linear(input_shape, hidden_size), ReLU(), Linear(hidden_size, output_shape))\n",
    "\n",
    "        def forward(self, data):\n",
    "            external_updated1_node_features = self._external1(data.x, data.edge_index, data.edge_attr)\n",
    "            external_updated2_node_features = self._external2(external_updated1_node_features, data.edge_index, data.edge_attr)\n",
    "            means_per_graph_external = scatter_mean(external_updated2_node_features, data.batch, dim=0)\n",
    "            graph_input = means_per_graph_external\n",
    "            z = self._graph_mlp(graph_input)\n",
    "            return z\n",
    "  ```\n",
    "- `class_weights` is used in classification tasks only, to assign class weights based on the training dataset content.\n",
    "- `cuda` and `ngpu` are used for indicating whether to use CUDA and how many GPUs. By default, CUDA is not used and `ngpu` is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = True # weighted loss function\n",
    "\n",
    "trainer = Trainer(\n",
    "    neuralnet = NaiveNetwork,\n",
    "    dataset_train = dataset_train,\n",
    "    dataset_val = dataset_val,\n",
    "    dataset_test = dataset_test,\n",
    "    class_weights = class_weights,\n",
    "    output_exporters = [HDF5OutputExporter(output_path)]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default optimizer is `torch.optim.Adam`. It is possible to specify optimizer's parameters or to use another PyTorch optimizer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD\n",
    "lr = 1e-3\n",
    "weight_decay = 0.001\n",
    "\n",
    "trainer.configure_optimizers(optimizer, lr, weight_decay)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default loss function for classification is `torch.nn.CrossEntropyLoss`. It is also possible to set other PyTorch loss functions by using `Trainer.set_lossfunction` method.\n",
    "\n",
    "Then we can train our model, using the `train()` method of the `Trainer` class.\n",
    "\n",
    "A few notes about `train()` method parameters:\n",
    "- `earlystop_patience`, `earlystop_maxgap` and `min_epoch` are used for controlling early stopping logic. `earlystop_patience` indicates the number of epochs after which the training ends if the validation loss as not improved. `earlystop_maxgap` indicated the maximum difference allowed between validation and training loss, and `min_epoch` is the minimum epoch to be reached before looking at maxgap.\n",
    "- `validate` is set True to perform validation on an independent dataset, which we called `dataset_val` few cells above.\n",
    "- `num_workers` can be set for indicating how many subprocesses to use for data loading. The default is 0 and it means that the data will be loaded in the main process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/giuliacrocioni/Desktop/docs/eScience/projects/3D-vac/deeprank-core/deeprankcore/utils/exporters.py:260: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->mixed,key->block1_values] [items->Index(['phase', 'entry', 'output'], dtype='object')]\n",
      "\n",
      "  self.df.to_hdf(\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'checkpoint_model' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/giuliacrocioni/Desktop/docs/eScience/projects/3D-vac/deeprank-core/tutorials/training_ppi.ipynb Cell 33\u001b[0m in \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/giuliacrocioni/Desktop/docs/eScience/projects/3D-vac/deeprank-core/tutorials/training_ppi.ipynb#X64sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m earlystop_maxgap \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/giuliacrocioni/Desktop/docs/eScience/projects/3D-vac/deeprank-core/tutorials/training_ppi.ipynb#X64sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m min_epoch \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/giuliacrocioni/Desktop/docs/eScience/projects/3D-vac/deeprank-core/tutorials/training_ppi.ipynb#X64sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/giuliacrocioni/Desktop/docs/eScience/projects/3D-vac/deeprank-core/tutorials/training_ppi.ipynb#X64sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     nepoch \u001b[39m=\u001b[39;49m epochs,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/giuliacrocioni/Desktop/docs/eScience/projects/3D-vac/deeprank-core/tutorials/training_ppi.ipynb#X64sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     batch_size \u001b[39m=\u001b[39;49m batch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giuliacrocioni/Desktop/docs/eScience/projects/3D-vac/deeprank-core/tutorials/training_ppi.ipynb#X64sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     earlystop_patience \u001b[39m=\u001b[39;49m earlystop_patience,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giuliacrocioni/Desktop/docs/eScience/projects/3D-vac/deeprank-core/tutorials/training_ppi.ipynb#X64sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     earlystop_maxgap \u001b[39m=\u001b[39;49m earlystop_maxgap,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giuliacrocioni/Desktop/docs/eScience/projects/3D-vac/deeprank-core/tutorials/training_ppi.ipynb#X64sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     min_epoch \u001b[39m=\u001b[39;49m min_epoch,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giuliacrocioni/Desktop/docs/eScience/projects/3D-vac/deeprank-core/tutorials/training_ppi.ipynb#X64sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     validate \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giuliacrocioni/Desktop/docs/eScience/projects/3D-vac/deeprank-core/tutorials/training_ppi.ipynb#X64sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     filename \u001b[39m=\u001b[39;49m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(output_path, \u001b[39m'\u001b[39;49m\u001b[39mmodel.pth.tar\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giuliacrocioni/Desktop/docs/eScience/projects/3D-vac/deeprank-core/tutorials/training_ppi.ipynb#X64sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m epoch \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mepoch_saved_model\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/giuliacrocioni/Desktop/docs/eScience/projects/3D-vac/deeprank-core/tutorials/training_ppi.ipynb#X64sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel saved at epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/docs/eScience/projects/3D-vac/deeprank-core/deeprankcore/trainer.py:663\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, nepoch, batch_size, shuffle, earlystop_patience, earlystop_maxgap, min_epoch, validate, num_workers, best_model, filename)\u001b[0m\n\u001b[1;32m    661\u001b[0m \u001b[39m# Now that the training loop is over, save the model\u001b[39;00m\n\u001b[1;32m    662\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[0;32m--> 663\u001b[0m     torch\u001b[39m.\u001b[39msave(checkpoint_model, filename)\n\u001b[1;32m    664\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopt_loaded_state_dict \u001b[39m=\u001b[39m checkpoint_model[\u001b[39m\"\u001b[39m\u001b[39moptimizer_state\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    665\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_load_state_dict \u001b[39m=\u001b[39m checkpoint_model[\u001b[39m\"\u001b[39m\u001b[39mmodel_state\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'checkpoint_model' referenced before assignment"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 8\n",
    "earlystop_patience = 5\n",
    "earlystop_maxgap = 0.1\n",
    "min_epoch = 10\n",
    "\n",
    "trainer.train(\n",
    "    nepoch = epochs,\n",
    "    batch_size = batch_size,\n",
    "    earlystop_patience = earlystop_patience,\n",
    "    earlystop_maxgap = earlystop_maxgap,\n",
    "    min_epoch = min_epoch,\n",
    "    validate = True,\n",
    "    filename = os.path.join(output_path, 'model.pth.tar'))\n",
    "\n",
    "epoch = trainer.epoch_saved_model\n",
    "print(f\"Model saved at epoch {epoch}\")\n",
    "pytorch_total_params = sum(p.numel() for p in trainer.model.parameters())\n",
    "print(f'Total # of parameters: {pytorch_total_params}')\n",
    "pytorch_trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
    "print(f'Total # of trainable parameters: {pytorch_trainable_params}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add test comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(batch_size = batch_size, num_workers = num_workers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add plot loss, e printa auc, f1 score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprankcore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
