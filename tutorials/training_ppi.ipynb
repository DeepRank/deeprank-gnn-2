{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepRank-Core basic Protein-Protein Interface training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example data used in this tutorial are available on [tdb, zenodo?](). To download the data, please [visit the dataset page for this workshop and click the “Download all” button. Unzip the downloaded file, and] save the contents as a folder called `data/` in the same directory of this notebook. (The name data and the folder location are optional but recommended, as this are the name and the location we will use to refer to the folder throughout the tutorial.)\n",
    "\n",
    "The dataset contains only 100 data points, which are obviously not enough to develop an impactful predictive model, and the scope of its use is indeed only demonstrative and informative for the users."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Software"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Follow the [updated instructions](https://github.com/DeepRank/deeprank-core#installation) in the README.md on the main branch for successfully installing `deeprankcore` package.\n",
    "2. To test the environment in which` deeprankcore` has been (successfully) installed, first clone [deeprank-core repository](https://github.com/DeepRank/deeprank-core). Navigate into it, and after having activated the environment and installed [pytest](https://anaconda.org/anaconda/pytest), run `pytest tests`. All tests should pass. We recommend installing `deeprankcore` and all its dependencies into a [conda](https://docs.conda.io/en/latest/) environment.\n",
    "3. Additionally, for this tutorial you need to install [scikit-learn](https://scikit-learn.org/stable/install.html) and [plotly](https://anaconda.org/plotly/plotly)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"margin-left: 1.5rem\" align=\"right\" src=\"images/training_ppi.png\" width=\"400\">\n",
    "\n",
    "This tutorial will demonstrate the use of DeepRank-Core for training Graph Neural Networks (GNNs) and Convolutional Neural Networks (CNNs) using Protein-Protein Interface (PPI) data for classification and regression predictive tasks.\n",
    "\n",
    "This tutorial assumes that the PPI data of interest have already been generated and saved into [HDF5 files](https://en.wikipedia.org/wiki/Hierarchical_Data_Format), with the data structure that DeepRank-Core expects. Such data have already been generated in `data/ppi/processed/` folder for the user, but for more details about the process please refer to the tutorial notebook [data_generation_ppi.ipynb](insert_link). It contains a detailed description of how such data are generated starting from the PDB files and gives instructions for generating different resolutions' data, i.e. residue- and atomic-level.\n",
    "\n",
    "This tutorial assumes also a basic knowledge of the [PyTorch](https://pytorch.org/) framework, on top of which the machine learning pipeline of DeepRank-Core has been developed. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"margin-right: 1.5rem\" align=\"left\" src=\"images/pmhc_pdb_example.png\" width=\"200\"/>\n",
    "\n",
    "The example dataset that we provide contains PDB files representing [Major Histocompatibility Complex (MHC) protein](https://en.wikipedia.org/wiki/Major_histocompatibility_complex) + peptide (pMHC) complexes, which play a key role in T-cell immunity. We are interested in predicting the Binding Affinity (BA) of the complexes, which can be used to determine the most suitable mutated tumor peptides as vaccine candidates.\n",
    "\n",
    "PDB models used in this tutorial have been generated with [PANDORA](https://github.com/X-lab-3D/PANDORA), an anchor restrained modeling pipeline for generating peptide-MHC structures. While target data, so the BA values for such pMHC complexes, have been retrieved from [MHCFlurry 2.0](https://data.mendeley.com/datasets/zx3kjzc3yx).\n",
    "\n",
    "On the left an example of a pMHC structure is shown, rendered using [ProteinViewer](https://marketplace.visualstudio.com/items?itemName=ArianJamasb.protein-viewer). The MHC protein is displayed in green, while the peptide is in orange."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the libraries needed for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import glob\n",
    "import os\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    f1_score)\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import numpy as np\n",
    "np.seterr(divide = 'ignore')\n",
    "np.seterr(invalid='ignore')\n",
    "import pandas as pd\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from deeprankcore.dataset import GraphDataset, GridDataset\n",
    "from deeprankcore.trainer import Trainer\n",
    "from deeprankcore.neuralnets.gnn.naive_gnn import NaiveNetwork\n",
    "from deeprankcore.neuralnets.cnn.model3d import CnnClassification\n",
    "from deeprankcore.utils.exporters import HDF5OutputExporter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths and sets\n",
    "\n",
    "Let's define the paths for reading the processed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"residue\"\n",
    "processed_data_path = os.path.join(\"data\", \"ppi\", \"processed\", level)\n",
    "input_data_path = glob.glob(os.path.join(processed_data_path, '*.hdf5'))\n",
    "output_path = os.path.join(\"data\", \"ppi\") # for saving predictions results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The levels refer to a different molecular resolution, either residue- or atomic-level. In residue-level graphs, each node represents one amino acid residue, while in atomic-level graphs each node represents one atom within the amino acid residues. In this tutorial, we will use residue-level data, but the same code could be applied to atomic-level data with no changes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's definea Pandas dataframe containing data points' IDs and the binary target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entry</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>residue-ppi:M-P:BA-102611</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>residue-ppi:M-P:BA-102669</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>residue-ppi:M-P:BA-102719</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>residue-ppi:M-P:BA-114468</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>residue-ppi:M-P:BA-115138</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       entry  target\n",
       "0  residue-ppi:M-P:BA-102611     1.0\n",
       "1  residue-ppi:M-P:BA-102669     0.0\n",
       "2  residue-ppi:M-P:BA-102719     0.0\n",
       "3  residue-ppi:M-P:BA-114468     0.0\n",
       "4  residue-ppi:M-P:BA-115138     0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict = {}\n",
    "df_dict['entry'] = []\n",
    "df_dict['target'] = []\n",
    "for fname in input_data_path:\n",
    "    with h5py.File(fname, 'r') as hdf5:\n",
    "        for mol in hdf5.keys():\n",
    "            target_value = float(hdf5[mol][\"target_values\"][\"binary\"][()])\n",
    "            df_dict['entry'].append(mol)\n",
    "            df_dict['target'].append(target_value)\n",
    "\n",
    "df = pd.DataFrame(data=df_dict)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As explained in [data_generation_ppi.ipynb tutorial](insert_link), for each data point there are two targets: \"BA\" and \"binary\". The first represents the continuous Binding Affinity value of the complex, while the second represents its binary representation, being 0 (BA > 500 nM) a not-binding complex and 1 (BA <= 500 nM) binding one.\n",
    "\n",
    "The dataframe `df` is used only to split data points into training, validation and test sets according to the \"binary\" target - using the target stratification, to keep the proportion of 0s and 1s sort of constant among the different sets. Training and validation set will be used during the training for updating the netowork's weights, while the test set will be held out as an indipendent test and will be used for later model's evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data statistics:\n",
      "\n",
      "Total samples: 100\n",
      "\n",
      "Training set: 72 samples, 72%\n",
      "\t- Class 0: 36 samples, 50%\n",
      "\t- Class 1: 36 samples, 50%\n",
      "Validation set: 18 samples, 18%\n",
      "\t- Class 0: 9 samples, 50%\n",
      "\t- Class 1: 9 samples, 50%\n",
      "Testing set: 10 samples, 10%\n",
      "\t- Class 0: 5 samples, 50%\n",
      "\t- Class 1: 5 samples, 50%\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.1, stratify=df.target, random_state=42)\n",
    "df_train, df_valid = train_test_split(df_train, test_size=0.2, stratify=df_train.target, random_state=42)\n",
    "\n",
    "print(f'Data statistics:\\n')\n",
    "print(f'Total samples: {len(df)}\\n')\n",
    "print(f'Training set: {len(df_train)} samples, {round(100*len(df_train)/len(df))}%')\n",
    "print(f'\\t- Class 0: {len(df_train[df_train.target == 0])} samples, {round(100*len(df_train[df_train.target == 0])/len(df_train))}%')\n",
    "print(f'\\t- Class 1: {len(df_train[df_train.target == 1])} samples, {round(100*len(df_train[df_train.target == 1])/len(df_train))}%')\n",
    "print(f'Validation set: {len(df_valid)} samples, {round(100*len(df_valid)/len(df))}%')\n",
    "print(f'\\t- Class 0: {len(df_valid[df_valid.target == 0])} samples, {round(100*len(df_valid[df_valid.target == 0])/len(df_valid))}%')\n",
    "print(f'\\t- Class 1: {len(df_valid[df_valid.target == 1])} samples, {round(100*len(df_valid[df_valid.target == 1])/len(df_valid))}%')\n",
    "print(f'Testing set: {len(df_test)} samples, {round(100*len(df_test)/len(df))}%')\n",
    "print(f'\\t- Class 0: {len(df_test[df_test.target == 0])} samples, {round(100*len(df_test[df_test.target == 0])/len(df_test))}%')\n",
    "print(f'\\t- Class 1: {len(df_test[df_test.target == 1])} samples, {round(100*len(df_test[df_test.target == 1])/len(df_test))}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification example\n",
    "\n",
    "Let's train a GNN and a CNN for a classification predictive task, which consists in predicting the \"binary\" target values. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GraphDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training GNNs the user can create `GraphDataset` instances. The latter class inherits from `DeeprankDataset` class, which in turns inherits from `Dataset` [PyTorch geometric class](https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/data/dataset.html), a base class for creating graph datasets.\n",
    "\n",
    "A few notes about `GraphDataset` parameters:\n",
    "- By default, all features contained in the HDF5 files are used, but the user can specify `node_features` and `edge_features` in `GraphDataset` if not all of them are needed. See the [docs](https://deeprankcore.readthedocs.io/en/latest/features.html) for more details about all the possible pre-implemented features. \n",
    "- For the `GraphDataset` class only it is possible to define a dictionary to indicate which transformations to apply to the features, being the transformations lambda functions and/or standardization. If `True`, standardization is applied after transformation, if the latter is present. Standardization consists in applying the following formula on each feature's value: ${x' = \\frac{x - \\mu}{\\sigma}}$, being ${\\mu}$ the mean and ${\\sigma}$ the standard deviation. Standardization is a scaling method where the values are centered around mean with a unit standard deviation. In the example below we will apply a logarithmic transformation and then standardization to all the features. It is also possible to use specific features as keys for indicating that we want to apply transformation and/or standardization to few features only. \n",
    "- Since we are applying standardization, we need to use training features' means and standard deviations to scale validation and test sets. For doing so, `train` and `dataset_train` parameters are used. When `train` is set `False`, a `dataset_train` of the same class must be provided and it will be used to scale the validation/testing set according to its features values. You need to pass `features_transform` to the training dataset only, since in other cases it will be ignored and only the one of `dataset_train` will be considered. \n",
    "- For regression, `task` can be assigned to `regress` and the `target` to `BA`, which is a continuous variable and this it is suitable for regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.dataset:\n",
      "Checking dataset Integrity...\n",
      "INFO:deeprankcore.dataset:Target classes set up to: [0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "   ['data/ppi/processed/residue/proc-28835.hdf5', 'data/ppi/processed/residue/proc-28842.hdf5', 'data/ppi/processed/residue/proc-28839.hdf5', 'data/ppi/processed/residue/proc-28838.hdf5', 'data/ppi/processed/residue/proc-28843.hdf5', 'data/ppi/processed/residue/proc-28834.hdf5', 'data/ppi/processed/residue/proc-28837.hdf5', 'data/ppi/processed/residue/proc-28840.hdf5', 'data/ppi/processed/residue/proc-28841.hdf5', 'data/ppi/processed/residue/proc-28836.hdf5'] dataset                 : 100%|██████████| 10/10 [00:00<00:00, 377.73it/s, entry_name=proc-28836.hdf5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.dataset:\n",
      "Checking dataset Integrity...\n",
      "INFO:deeprankcore.dataset:Target classes set up to: [0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading validation data...\n",
      "   ['data/ppi/processed/residue/proc-28835.hdf5', 'data/ppi/processed/residue/proc-28842.hdf5', 'data/ppi/processed/residue/proc-28839.hdf5', 'data/ppi/processed/residue/proc-28838.hdf5', 'data/ppi/processed/residue/proc-28843.hdf5', 'data/ppi/processed/residue/proc-28834.hdf5', 'data/ppi/processed/residue/proc-28837.hdf5', 'data/ppi/processed/residue/proc-28840.hdf5', 'data/ppi/processed/residue/proc-28841.hdf5', 'data/ppi/processed/residue/proc-28836.hdf5'] dataset                 : 100%|██████████| 10/10 [00:00<00:00, 893.49it/s, entry_name=proc-28836.hdf5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:deeprankcore.dataset:features_transform parameter, if set, will be ignored.\n",
      "features_transform will remain the same as the one used in training phase.\n",
      "INFO:deeprankcore.dataset:\n",
      "Checking dataset Integrity...\n",
      "INFO:deeprankcore.dataset:Target classes set up to: [0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading test data...\n",
      "   ['data/ppi/processed/residue/proc-28835.hdf5', 'data/ppi/processed/residue/proc-28842.hdf5', 'data/ppi/processed/residue/proc-28839.hdf5', 'data/ppi/processed/residue/proc-28838.hdf5', 'data/ppi/processed/residue/proc-28843.hdf5', 'data/ppi/processed/residue/proc-28834.hdf5', 'data/ppi/processed/residue/proc-28837.hdf5', 'data/ppi/processed/residue/proc-28840.hdf5', 'data/ppi/processed/residue/proc-28841.hdf5', 'data/ppi/processed/residue/proc-28836.hdf5'] dataset                 : 100%|██████████| 10/10 [00:00<00:00, 1168.92it/s, entry_name=proc-28836.hdf5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:deeprankcore.dataset:features_transform parameter, if set, will be ignored.\n",
      "features_transform will remain the same as the one used in training phase.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target = \"binary\"\n",
    "task = \"classif\"\n",
    "features_transform = {'all': {'transform': lambda x: np.cbrt(x), 'standardize': True}}\n",
    "\n",
    "print('Loading training data...')\n",
    "dataset_train = GraphDataset(\n",
    "    hdf5_path = input_data_path,\n",
    "    subset = list(df_train.entry), # selects only data points with ids in df_train.entry\n",
    "    target = target,\n",
    "    task = task,\n",
    "    features_transform = features_transform\n",
    ")\n",
    "print('\\nLoading validation data...')\n",
    "dataset_val = GraphDataset(\n",
    "    hdf5_path = input_data_path,\n",
    "    subset = list(df_valid.entry), # selects only data points with ids in df_valid.entry\n",
    "    target = target,\n",
    "    task = task,\n",
    "    train = False,\n",
    "    dataset_train = dataset_train\n",
    ")\n",
    "print('\\nLoading test data...')\n",
    "dataset_test = GraphDataset(\n",
    "    hdf5_path = input_data_path,\n",
    "    subset = list(df_test.entry), # selects only data points with ids in df_test.entry\n",
    "    target = target,\n",
    "    task = task,\n",
    "    train = False,\n",
    "    dataset_train = dataset_train\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer\n",
    "\n",
    "The class `Trainer` implements training, validation and testing of PyTorch-based neural networks. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes about `Trainer` parameters:\n",
    "- `neuralnet` can be whatever neural network class which inherits from `torch.nn.Module`, and it shouldn't be specific to regression or classification in terms of output shape. `Trainer` class takes care of formatting the output shape according to the task. In this tutorial we will use `NaiveNetwork` (in `deeprankcore.neuralnets.gnn.naive_gnn`), whose architecture is shown below:\n",
    "  ```python\n",
    "  class NaiveConvolutionalLayer(Module):\n",
    "\n",
    "    def __init__(self, count_node_features, count_edge_features):\n",
    "        super().__init__()\n",
    "        message_size = 32\n",
    "        edge_input_size = 2 * count_node_features + count_edge_features\n",
    "        self._edge_mlp = Sequential(Linear(edge_input_size, message_size), ReLU())\n",
    "        node_input_size = count_node_features + message_size\n",
    "        self._node_mlp = Sequential(Linear(node_input_size, count_node_features), ReLU())\n",
    "\n",
    "    def forward(self, node_features, edge_node_indices, edge_features):\n",
    "        # generate messages over edges\n",
    "        node0_indices, node1_indices = edge_node_indices\n",
    "        node0_features = node_features[node0_indices]\n",
    "        node1_features = node_features[node1_indices]\n",
    "        message_input = torch.cat([node0_features, node1_features, edge_features], dim=1)\n",
    "        messages_per_neighbour = self._edge_mlp(message_input)\n",
    "        # aggregate messages\n",
    "        out = torch.zeros(node_features.shape[0], messages_per_neighbour.shape[1]).to(node_features.device)\n",
    "        message_sums_per_node = scatter_sum(messages_per_neighbour, node0_indices, dim=0, out=out)\n",
    "        # update nodes\n",
    "        node_input = torch.cat([node_features, message_sums_per_node], dim=1)\n",
    "        node_output = self._node_mlp(node_input)\n",
    "        return node_output\n",
    "\n",
    "    class NaiveNetwork(Module):\n",
    "\n",
    "        def __init__(self, input_shape: int, output_shape: int, input_shape_edge: int):\n",
    "            \"\"\"\n",
    "            Args:\n",
    "                input_shape (int): Number of node input features.\n",
    "                output_shape (int): Number of output value per graph.\n",
    "                input_shape_edge (int): Number of edge input features.\n",
    "            \"\"\"\n",
    "            super().__init__()\n",
    "            self._external1 = NaiveConvolutionalLayer(input_shape, input_shape_edge)\n",
    "            self._external2 = NaiveConvolutionalLayer(input_shape, input_shape_edge)\n",
    "            hidden_size = 128\n",
    "            self._graph_mlp = Sequential(Linear(input_shape, hidden_size), ReLU(), Linear(hidden_size, output_shape))\n",
    "\n",
    "        def forward(self, data):\n",
    "            external_updated1_node_features = self._external1(data.x, data.edge_index, data.edge_attr)\n",
    "            external_updated2_node_features = self._external2(external_updated1_node_features, data.edge_index, data.edge_attr)\n",
    "            means_per_graph_external = scatter_mean(external_updated2_node_features, data.batch, dim=0)\n",
    "            graph_input = means_per_graph_external\n",
    "            z = self._graph_mlp(graph_input)\n",
    "            return z\n",
    "  ```\n",
    "- `class_weights` is used in classification tasks only, to assign class weights based on the training dataset content. In this case we have a balanced dataset (50% 0 and 50% 1), so it doesn't make sense to use it. It defaults to False. \n",
    "- `cuda` and `ngpu` are used for indicating whether to use CUDA and how many GPUs. By default, CUDA is not used and `ngpu` is 0.\n",
    "- The user can specify a deeprankcore exporter or a custom one in `output_exporters` parameter, together with the path where to save the results. Exporters are used for storing predictions information collected later on during training and testing. We will see later how to read the results saved by `HDF5OutputExporter`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.trainer:Device set to cpu.\n",
      "INFO:deeprankcore.trainer:No loss function provided, the default loss function for classif tasks is used: <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    neuralnet = NaiveNetwork,\n",
    "    dataset_train = dataset_train,\n",
    "    dataset_val = dataset_val,\n",
    "    dataset_test = dataset_test,\n",
    "    output_exporters = [HDF5OutputExporter(os.path.join(output_path, \"gnn_classif\"))]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default optimizer is `torch.optim.Adam`. It is possible to specify optimizer's parameters or to use another PyTorch optimizer object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD\n",
    "lr = 1e-3\n",
    "weight_decay = 0.001\n",
    "\n",
    "trainer.configure_optimizers(optimizer, lr, weight_decay)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default loss function for classification is `torch.nn.CrossEntropyLoss`, while for regression is `torch.nn.MSELoss`. It is also possible to set other PyTorch loss functions by using `Trainer.set_lossfunction` method.\n",
    "\n",
    "Then we can train our model, using the `train()` method of the `Trainer` class.\n",
    "\n",
    "A few notes about `train()` method parameters:\n",
    "- `earlystop_patience`, `earlystop_maxgap` and `min_epoch` are used for controlling early stopping logic. `earlystop_patience` indicates the number of epochs after which the training ends if the validation loss as not improved. `earlystop_maxgap` indicated the maximum difference allowed between validation and training loss, and `min_epoch` is the minimum epoch to be reached before looking at maxgap.\n",
    "- `validate` is set True to perform validation on an independent dataset, which we called `dataset_val` few cells above.\n",
    "- `num_workers` can be set for indicating how many subprocesses to use for data loading. The default is 0 and it means that the data will be loaded in the main process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.trainer:Training set loaded\n",
      "\n",
      "INFO:deeprankcore.trainer:Validation set loaded\n",
      "\n",
      "INFO:deeprankcore.trainer:Epoch 0:\n",
      "INFO:deeprankcore.trainer:training loss 30.975495020548504 | time 0.49440479278564453\n",
      "INFO:deeprankcore.trainer:validation loss 30.820761574639214 | time 0.13362717628479004\n",
      "INFO:deeprankcore.trainer:Epoch 1:\n",
      "INFO:deeprankcore.trainer:training loss 10.772469229168362 | time 1.207261085510254\n",
      "INFO:deeprankcore.trainer:validation loss 0.9031418363253275 | time 0.18108797073364258\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 1.\n",
      "INFO:deeprankcore.trainer:Epoch 2:\n",
      "INFO:deeprankcore.trainer:training loss 1.0082799560493894 | time 1.3351099491119385\n",
      "INFO:deeprankcore.trainer:validation loss 0.7291429241498312 | time 0.11986494064331055\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 2.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.903142 --> 0.729143).\n",
      "INFO:deeprankcore.trainer:Epoch 3:\n",
      "INFO:deeprankcore.trainer:training loss 0.8918893668386672 | time 1.2460148334503174\n",
      "INFO:deeprankcore.trainer:validation loss 0.7124772932794359 | time 0.10070109367370605\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 3.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.729143 --> 0.712477).\n",
      "INFO:deeprankcore.trainer:Epoch 4:\n",
      "INFO:deeprankcore.trainer:training loss 0.7241496245066324 | time 1.4033689498901367\n",
      "INFO:deeprankcore.trainer:validation loss 0.7297427521811591 | time 0.19195818901062012\n",
      "INFO:deeprankcore.trainer:Validation loss did not decrease (0.712477 --> 0.729743). EarlyStopping counter: 1 out of 5\n",
      "INFO:deeprankcore.trainer:Epoch 5:\n",
      "INFO:deeprankcore.trainer:training loss 0.6961668928464254 | time 1.8007419109344482\n",
      "INFO:deeprankcore.trainer:validation loss 0.7017690738042196 | time 0.16534900665283203\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 5.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.712477 --> 0.701769).\n",
      "INFO:deeprankcore.trainer:Epoch 6:\n",
      "INFO:deeprankcore.trainer:training loss 0.7632920410897996 | time 1.7617378234863281\n",
      "INFO:deeprankcore.trainer:validation loss 0.6988125244776408 | time 0.22299480438232422\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 6.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.701769 --> 0.698813).\n",
      "INFO:deeprankcore.trainer:Epoch 7:\n",
      "INFO:deeprankcore.trainer:training loss 0.7253633605109321 | time 1.2863686084747314\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 8\n",
    "earlystop_patience = 5\n",
    "earlystop_maxgap = 0.1\n",
    "min_epoch = 10\n",
    "\n",
    "trainer.train(\n",
    "    nepoch = epochs,\n",
    "    batch_size = batch_size,\n",
    "    earlystop_patience = earlystop_patience,\n",
    "    earlystop_maxgap = earlystop_maxgap,\n",
    "    min_epoch = min_epoch,\n",
    "    validate = True,\n",
    "    filename = os.path.join(output_path, \"gnn_classif\", \"model.pth.tar\"))\n",
    "\n",
    "epoch = trainer.epoch_saved_model\n",
    "print(f\"Model saved at epoch {epoch}\")\n",
    "pytorch_total_params = sum(p.numel() for p in trainer.model.parameters())\n",
    "print(f'Total # of parameters: {pytorch_total_params}')\n",
    "pytorch_trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
    "print(f'Total # of trainable parameters: {pytorch_trainable_params}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing\n",
    "\n",
    "And we can test the trained model on our `dataset_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.trainer:Loading independent testing dataset...\n",
      "INFO:deeprankcore.trainer:Testing set loaded\n",
      "\n",
      "INFO:deeprankcore.trainer:testing loss 0.6990455985069275 | time 0.12017321586608887\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results visualization\n",
    "\n",
    "Finally, we can inspect the results saved by `HDF5OutputExporter`, which can be found in `data/ppi/gnn_classif/` folder in the form of an HDF5 file, `output_exporter.hdf5`. Note that the folder contains the saved pre-trained model as well. \n",
    "\n",
    "`output_exporter.hdf5` contains [HDF5 Groups](https://docs.h5py.org/en/stable/high/group.html) which refer to each phase, e.g. training and testing if both are run, only one of them otherwise. Training phase includes validation results as well. This HDF5 file can be read as a Pandas Dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase</th>\n",
       "      <th>epoch</th>\n",
       "      <th>entry</th>\n",
       "      <th>output</th>\n",
       "      <th>target</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-119313</td>\n",
       "      <td>[3.7293703281787716e-13, 1.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.40311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-118225</td>\n",
       "      <td>[5.227542032061605e-13, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.40311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-151990</td>\n",
       "      <td>[7.269274022786085e-14, 1.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.40311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-113341</td>\n",
       "      <td>[6.299188059187555e-13, 1.0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14.40311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-115586</td>\n",
       "      <td>[1.851313427645579e-13, 1.0]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.40311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phase  epoch                      entry                         output  \\\n",
       "0  training    0.0  residue-ppi:M-P:BA-119313  [3.7293703281787716e-13, 1.0]   \n",
       "1  training    0.0  residue-ppi:M-P:BA-118225   [5.227542032061605e-13, 1.0]   \n",
       "2  training    0.0  residue-ppi:M-P:BA-151990   [7.269274022786085e-14, 1.0]   \n",
       "3  training    0.0  residue-ppi:M-P:BA-113341   [6.299188059187555e-13, 1.0]   \n",
       "4  training    0.0  residue-ppi:M-P:BA-115586   [1.851313427645579e-13, 1.0]   \n",
       "\n",
       "   target      loss  \n",
       "0     0.0  14.40311  \n",
       "1     1.0  14.40311  \n",
       "2     0.0  14.40311  \n",
       "3     1.0  14.40311  \n",
       "4     0.0  14.40311  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_train = pd.read_hdf(os.path.join(output_path, \"gnn_classif\", \"output_exporter.hdf5\"), key=\"training\")\n",
    "output_test = pd.read_hdf(os.path.join(output_path, \"gnn_classif\", \"output_exporter.hdf5\"), key=\"testing\")\n",
    "output_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframes contain `phase`, `epoch`, `entry`, `output`, `target`, and `loss` columns, and can be easily used to visualize the results.\n",
    "\n",
    "Let's plot for example the loss across the epochs for the training and the validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "phase=training<br>epoch=%{x}<br>loss=%{y}<extra></extra>",
         "legendgroup": "training",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "training",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20
         ],
         "xaxis": "x",
         "y": [
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          14.403110053804186,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          4.074258353975084,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7943568362130059,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7419716119766235,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.7154281602965461,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.758401354153951,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7041120992766486,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7049471139907837,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7226497729619344,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.7085522413253784,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.705119616455502,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.6953606340620253,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.7015070186720954,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6794619162877401,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.6791796021991305,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.712208178308275,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6887297564082675,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6985453367233276,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6941802965270149,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6725593672858344,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729,
          0.6895554131931729
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "phase=validation<br>epoch=%{x}<br>loss=%{y}<extra></extra>",
         "legendgroup": "validation",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines+markers",
         "name": "validation",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20
         ],
         "xaxis": "x",
         "y": [
          14.66484906938341,
          14.66484906938341,
          14.66484906938341,
          14.66484906938341,
          14.66484906938341,
          14.66484906938341,
          14.66484906938341,
          14.66484906938341,
          14.66484906938341,
          14.66484906938341,
          14.66484906938341,
          14.66484906938341,
          14.66484906938341,
          14.66484906938341,
          14.66484906938341,
          14.66484906938341,
          14.66484906938341,
          14.66484906938341,
          0.7239808506435819,
          0.7239808506435819,
          0.7239808506435819,
          0.7239808506435819,
          0.7239808506435819,
          0.7239808506435819,
          0.7239808506435819,
          0.7239808506435819,
          0.7239808506435819,
          0.7239808506435819,
          0.7239808506435819,
          0.7239808506435819,
          0.7239808506435819,
          0.7239808506435819,
          0.7239808506435819,
          0.7239808506435819,
          0.7239808506435819,
          0.7239808506435819,
          0.8548417687416077,
          0.8548417687416077,
          0.8548417687416077,
          0.8548417687416077,
          0.8548417687416077,
          0.8548417687416077,
          0.8548417687416077,
          0.8548417687416077,
          0.8548417687416077,
          0.8548417687416077,
          0.8548417687416077,
          0.8548417687416077,
          0.8548417687416077,
          0.8548417687416077,
          0.8548417687416077,
          0.8548417687416077,
          0.8548417687416077,
          0.8548417687416077,
          0.7036890983581543,
          0.7036890983581543,
          0.7036890983581543,
          0.7036890983581543,
          0.7036890983581543,
          0.7036890983581543,
          0.7036890983581543,
          0.7036890983581543,
          0.7036890983581543,
          0.7036890983581543,
          0.7036890983581543,
          0.7036890983581543,
          0.7036890983581543,
          0.7036890983581543,
          0.7036890983581543,
          0.7036890983581543,
          0.7036890983581543,
          0.7036890983581543,
          0.6979213356971741,
          0.6979213356971741,
          0.6979213356971741,
          0.6979213356971741,
          0.6979213356971741,
          0.6979213356971741,
          0.6979213356971741,
          0.6979213356971741,
          0.6979213356971741,
          0.6979213356971741,
          0.6979213356971741,
          0.6979213356971741,
          0.6979213356971741,
          0.6979213356971741,
          0.6979213356971741,
          0.6979213356971741,
          0.6979213356971741,
          0.6979213356971741,
          0.7076743973626031,
          0.7076743973626031,
          0.7076743973626031,
          0.7076743973626031,
          0.7076743973626031,
          0.7076743973626031,
          0.7076743973626031,
          0.7076743973626031,
          0.7076743973626031,
          0.7076743973626031,
          0.7076743973626031,
          0.7076743973626031,
          0.7076743973626031,
          0.7076743973626031,
          0.7076743973626031,
          0.7076743973626031,
          0.7076743973626031,
          0.7076743973626031,
          0.6792056692971123,
          0.6792056692971123,
          0.6792056692971123,
          0.6792056692971123,
          0.6792056692971123,
          0.6792056692971123,
          0.6792056692971123,
          0.6792056692971123,
          0.6792056692971123,
          0.6792056692971123,
          0.6792056692971123,
          0.6792056692971123,
          0.6792056692971123,
          0.6792056692971123,
          0.6792056692971123,
          0.6792056692971123,
          0.6792056692971123,
          0.6792056692971123,
          0.666326277785831,
          0.666326277785831,
          0.666326277785831,
          0.666326277785831,
          0.666326277785831,
          0.666326277785831,
          0.666326277785831,
          0.666326277785831,
          0.666326277785831,
          0.666326277785831,
          0.666326277785831,
          0.666326277785831,
          0.666326277785831,
          0.666326277785831,
          0.666326277785831,
          0.666326277785831,
          0.666326277785831,
          0.666326277785831,
          0.6768083837297227,
          0.6768083837297227,
          0.6768083837297227,
          0.6768083837297227,
          0.6768083837297227,
          0.6768083837297227,
          0.6768083837297227,
          0.6768083837297227,
          0.6768083837297227,
          0.6768083837297227,
          0.6768083837297227,
          0.6768083837297227,
          0.6768083837297227,
          0.6768083837297227,
          0.6768083837297227,
          0.6768083837297227,
          0.6768083837297227,
          0.6768083837297227,
          0.6613771518071493,
          0.6613771518071493,
          0.6613771518071493,
          0.6613771518071493,
          0.6613771518071493,
          0.6613771518071493,
          0.6613771518071493,
          0.6613771518071493,
          0.6613771518071493,
          0.6613771518071493,
          0.6613771518071493,
          0.6613771518071493,
          0.6613771518071493,
          0.6613771518071493,
          0.6613771518071493,
          0.6613771518071493,
          0.6613771518071493,
          0.6613771518071493,
          0.6629884375466241,
          0.6629884375466241,
          0.6629884375466241,
          0.6629884375466241,
          0.6629884375466241,
          0.6629884375466241,
          0.6629884375466241,
          0.6629884375466241,
          0.6629884375466241,
          0.6629884375466241,
          0.6629884375466241,
          0.6629884375466241,
          0.6629884375466241,
          0.6629884375466241,
          0.6629884375466241,
          0.6629884375466241,
          0.6629884375466241,
          0.6629884375466241,
          0.6967042949464586,
          0.6967042949464586,
          0.6967042949464586,
          0.6967042949464586,
          0.6967042949464586,
          0.6967042949464586,
          0.6967042949464586,
          0.6967042949464586,
          0.6967042949464586,
          0.6967042949464586,
          0.6967042949464586,
          0.6967042949464586,
          0.6967042949464586,
          0.6967042949464586,
          0.6967042949464586,
          0.6967042949464586,
          0.6967042949464586,
          0.6967042949464586,
          0.6581047243542142,
          0.6581047243542142,
          0.6581047243542142,
          0.6581047243542142,
          0.6581047243542142,
          0.6581047243542142,
          0.6581047243542142,
          0.6581047243542142,
          0.6581047243542142,
          0.6581047243542142,
          0.6581047243542142,
          0.6581047243542142,
          0.6581047243542142,
          0.6581047243542142,
          0.6581047243542142,
          0.6581047243542142,
          0.6581047243542142,
          0.6581047243542142,
          0.6519638962215848,
          0.6519638962215848,
          0.6519638962215848,
          0.6519638962215848,
          0.6519638962215848,
          0.6519638962215848,
          0.6519638962215848,
          0.6519638962215848,
          0.6519638962215848,
          0.6519638962215848,
          0.6519638962215848,
          0.6519638962215848,
          0.6519638962215848,
          0.6519638962215848,
          0.6519638962215848,
          0.6519638962215848,
          0.6519638962215848,
          0.6519638962215848,
          0.6546443502108256,
          0.6546443502108256,
          0.6546443502108256,
          0.6546443502108256,
          0.6546443502108256,
          0.6546443502108256,
          0.6546443502108256,
          0.6546443502108256,
          0.6546443502108256,
          0.6546443502108256,
          0.6546443502108256,
          0.6546443502108256,
          0.6546443502108256,
          0.6546443502108256,
          0.6546443502108256,
          0.6546443502108256,
          0.6546443502108256,
          0.6546443502108256,
          0.652233726448483,
          0.652233726448483,
          0.652233726448483,
          0.652233726448483,
          0.652233726448483,
          0.652233726448483,
          0.652233726448483,
          0.652233726448483,
          0.652233726448483,
          0.652233726448483,
          0.652233726448483,
          0.652233726448483,
          0.652233726448483,
          0.652233726448483,
          0.652233726448483,
          0.652233726448483,
          0.652233726448483,
          0.652233726448483,
          0.6648412413067288,
          0.6648412413067288,
          0.6648412413067288,
          0.6648412413067288,
          0.6648412413067288,
          0.6648412413067288,
          0.6648412413067288,
          0.6648412413067288,
          0.6648412413067288,
          0.6648412413067288,
          0.6648412413067288,
          0.6648412413067288,
          0.6648412413067288,
          0.6648412413067288,
          0.6648412413067288,
          0.6648412413067288,
          0.6648412413067288,
          0.6648412413067288,
          0.66408681207233,
          0.66408681207233,
          0.66408681207233,
          0.66408681207233,
          0.66408681207233,
          0.66408681207233,
          0.66408681207233,
          0.66408681207233,
          0.66408681207233,
          0.66408681207233,
          0.66408681207233,
          0.66408681207233,
          0.66408681207233,
          0.66408681207233,
          0.66408681207233,
          0.66408681207233,
          0.66408681207233,
          0.66408681207233,
          0.6467982795503404,
          0.6467982795503404,
          0.6467982795503404,
          0.6467982795503404,
          0.6467982795503404,
          0.6467982795503404,
          0.6467982795503404,
          0.6467982795503404,
          0.6467982795503404,
          0.6467982795503404,
          0.6467982795503404,
          0.6467982795503404,
          0.6467982795503404,
          0.6467982795503404,
          0.6467982795503404,
          0.6467982795503404,
          0.6467982795503404,
          0.6467982795503404,
          0.6522389517890083,
          0.6522389517890083,
          0.6522389517890083,
          0.6522389517890083,
          0.6522389517890083,
          0.6522389517890083,
          0.6522389517890083,
          0.6522389517890083,
          0.6522389517890083,
          0.6522389517890083,
          0.6522389517890083,
          0.6522389517890083,
          0.6522389517890083,
          0.6522389517890083,
          0.6522389517890083,
          0.6522389517890083,
          0.6522389517890083,
          0.6522389517890083,
          0.6456168625089858,
          0.6456168625089858,
          0.6456168625089858,
          0.6456168625089858,
          0.6456168625089858,
          0.6456168625089858,
          0.6456168625089858,
          0.6456168625089858,
          0.6456168625089858,
          0.6456168625089858,
          0.6456168625089858,
          0.6456168625089858,
          0.6456168625089858,
          0.6456168625089858,
          0.6456168625089858,
          0.6456168625089858,
          0.6456168625089858,
          0.6456168625089858
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "height": 400,
        "legend": {
         "title": {
          "text": "phase"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "shapes": [
         {
          "line": {
           "color": "green",
           "dash": "dash",
           "width": 3
          },
          "type": "line",
          "x0": 20,
          "x1": 20,
          "xref": "x",
          "y0": 0,
          "y1": 1,
          "yref": "y domain"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Loss vs epochs"
        },
        "width": 700,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch #"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.line(\n",
    "    output_train,\n",
    "    x='epoch',\n",
    "    y='loss',\n",
    "    color='phase',\n",
    "    markers=True)\n",
    "\n",
    "fig.add_vline(x=trainer.epoch_saved_model, line_width=3, line_dash=\"dash\", line_color=\"green\")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Epoch #',\n",
    "    yaxis_title='Loss',\n",
    "    title='Loss vs epochs - GNN training',\n",
    "    width=700, height=400,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's print a few metrics of interest for classification tasks: the Area under the ROC curve (AUC), and for a threshold of 0.5 the precision, the recall, the accuracy and the f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for training:\n",
      "AUC: 0.6\n",
      "Considering a threshold of 0.5\n",
      "- Precision: 0.6\n",
      "- Recall: 0.4\n",
      "- Accuracy: 0.6\n",
      "- F1: 0.5\n",
      "\n",
      "Metrics for validation:\n",
      "AUC: 0.9\n",
      "Considering a threshold of 0.5\n",
      "- Precision: 0.7\n",
      "- Recall: 0.9\n",
      "- Accuracy: 0.8\n",
      "- F1: 0.8\n",
      "\n",
      "Metrics for testing:\n",
      "AUC: 0.4\n",
      "Considering a threshold of 0.5\n",
      "- Precision: 0.5\n",
      "- Recall: 0.8\n",
      "- Accuracy: 0.5\n",
      "- F1: 0.6\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "df = pd.concat([output_train, output_test])\n",
    "df_plot = df[(df.epoch == trainer.epoch_saved_model) | ((df.epoch == trainer.epoch_saved_model) & (df.phase == 'testing'))]\n",
    "\n",
    "for idx, set in enumerate(['training', 'validation', 'testing']):\n",
    "    df_plot_phase = df_plot[(df_plot.phase == set)]\n",
    "    y_true = df_plot_phase.target\n",
    "    y_score = np.array(df_plot_phase.output.values.tolist())[:, 1]\n",
    "\n",
    "    print(f'\\nMetrics for {set}:')\n",
    "    fpr_roc, tpr_roc, thr_roc = roc_curve(y_true, y_score)\n",
    "    auc_score = auc(fpr_roc, tpr_roc)\n",
    "    print(f'AUC: {round(auc_score, 1)}')\n",
    "    print(f'Considering a threshold of {threshold}')\n",
    "    y_pred = (y_score > threshold)*1\n",
    "    print(f'- Precision: {round(precision_score(y_true, y_pred), 1)}')\n",
    "    print(f'- Recall: {round(recall_score(y_true, y_pred), 1)}')\n",
    "    print(f'- Accuracy: {round(accuracy_score(y_true, y_pred), 1)}')\n",
    "    print(f'- F1: {round(f1_score(y_true, y_pred), 1)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course such metrics results are not satisfying from the performance point of view, but we did use only 100 data points, which of course is not enough to train a neural network. Now let's do the same exercise but using grids instead of graphs and CNNs instead of GNNs. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GridDataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training CNNs the user can create `GridDataset` instances.\n",
    "\n",
    "A few notes about `GridDataset` parameters:\n",
    "- By default, all features contained in the HDF5 files are used, but the user can specify `features` in `GridDataset` if not all of them are needed. Since grids features are derived from node and edge features mapped from graphs to grid, the easiest way to see which features are available is to look at the HDF5 file, as explained in detail in `data_generation_ppi.ipynb`, section \"Other tools\".  \n",
    "- As for graphs, if we want to perform regression, `task` can be assigned to `regress` and the `target` to `BA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.dataset:\n",
      "Checking dataset Integrity...\n",
      "INFO:deeprankcore.dataset:Target classes set up to: [0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "   ['data/ppi/processed/residue/proc-28835.hdf5', 'data/ppi/processed/residue/proc-28842.hdf5', 'data/ppi/processed/residue/proc-28839.hdf5', 'data/ppi/processed/residue/proc-28838.hdf5', 'data/ppi/processed/residue/proc-28843.hdf5', 'data/ppi/processed/residue/proc-28834.hdf5', 'data/ppi/processed/residue/proc-28837.hdf5', 'data/ppi/processed/residue/proc-28840.hdf5', 'data/ppi/processed/residue/proc-28841.hdf5', 'data/ppi/processed/residue/proc-28836.hdf5'] dataset                 : 100%|██████████| 10/10 [00:00<00:00, 274.13it/s, entry_name=proc-28836.hdf5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.dataset:\n",
      "Checking dataset Integrity...\n",
      "INFO:deeprankcore.dataset:Target classes set up to: [0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading validation data...\n",
      "   ['data/ppi/processed/residue/proc-28835.hdf5', 'data/ppi/processed/residue/proc-28842.hdf5', 'data/ppi/processed/residue/proc-28839.hdf5', 'data/ppi/processed/residue/proc-28838.hdf5', 'data/ppi/processed/residue/proc-28843.hdf5', 'data/ppi/processed/residue/proc-28834.hdf5', 'data/ppi/processed/residue/proc-28837.hdf5', 'data/ppi/processed/residue/proc-28840.hdf5', 'data/ppi/processed/residue/proc-28841.hdf5', 'data/ppi/processed/residue/proc-28836.hdf5'] dataset                 : 100%|██████████| 10/10 [00:00<00:00, 698.57it/s, entry_name=proc-28836.hdf5]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.dataset:\n",
      "Checking dataset Integrity...\n",
      "INFO:deeprankcore.dataset:Target classes set up to: [0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Loading test data...\n",
      "   ['data/ppi/processed/residue/proc-28835.hdf5', 'data/ppi/processed/residue/proc-28842.hdf5', 'data/ppi/processed/residue/proc-28839.hdf5', 'data/ppi/processed/residue/proc-28838.hdf5', 'data/ppi/processed/residue/proc-28843.hdf5', 'data/ppi/processed/residue/proc-28834.hdf5', 'data/ppi/processed/residue/proc-28837.hdf5', 'data/ppi/processed/residue/proc-28840.hdf5', 'data/ppi/processed/residue/proc-28841.hdf5', 'data/ppi/processed/residue/proc-28836.hdf5'] dataset                 : 100%|██████████| 10/10 [00:00<00:00, 951.56it/s, entry_name=proc-28836.hdf5]\n"
     ]
    }
   ],
   "source": [
    "target = \"binary\"\n",
    "task = \"classif\"\n",
    "\n",
    "print('Loading training data...')\n",
    "dataset_train = GridDataset(\n",
    "    hdf5_path = input_data_path,\n",
    "    subset = list(df_train.entry), # selects only data points with ids in df_train.entry\n",
    "    target = target,\n",
    "    task = task\n",
    ")\n",
    "print('\\nLoading validation data...')\n",
    "dataset_val = GridDataset(\n",
    "    hdf5_path = input_data_path,\n",
    "    subset = list(df_valid.entry), # selects only data points with ids in df_valid.entry\n",
    "    target = target,\n",
    "    task = task\n",
    ")\n",
    "print('\\nLoading test data...')\n",
    "dataset_test = GridDataset(\n",
    "    hdf5_path = input_data_path,\n",
    "    subset = list(df_test.entry), # selects only data points with ids in df_test.entry\n",
    "    target = target,\n",
    "    task = task\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer\n",
    "\n",
    "As for graphs, the class `Trainer` is used for training, validation and testing of the PyTorch-based CNN. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Also in this case, `neuralnet` can be whatever neural network class which inherits from `torch.nn.Module`, and it shouldn't be specific to regression or classification in terms of output shape. Here we use `CnnClassification` (in `deeprankcore.neuralnets.cnn.model3d`), whose architecture is shown below:\n",
    "  ```python\n",
    "    class CnnClassification(torch.nn.Module):\n",
    "\n",
    "        def __init__(self, num_features, box_shape):\n",
    "            super().__init__()\n",
    "\n",
    "            self.convlayer_000 = torch.nn.Conv3d(num_features, 4, kernel_size=2)\n",
    "            self.convlayer_001 = torch.nn.MaxPool3d((2, 2, 2))\n",
    "            self.convlayer_002 = torch.nn.Conv3d(4, 5, kernel_size=2)\n",
    "            self.convlayer_003 = torch.nn.MaxPool3d((2, 2, 2))\n",
    "\n",
    "            size = self._get_conv_output(num_features, box_shape)\n",
    "\n",
    "            self.fclayer_000 = torch.nn.Linear(size, 84)\n",
    "            self.fclayer_001 = torch.nn.Linear(84, 2)\n",
    "\n",
    "        def _get_conv_output(self, num_features, shape):\n",
    "            inp = Variable(torch.rand(1, num_features, *shape))\n",
    "            out = self._forward_features(inp)\n",
    "            return out.data.view(1, -1).size(1)\n",
    "\n",
    "        def _forward_features(self, x):\n",
    "            x = F.relu(self.convlayer_000(x))\n",
    "            x = self.convlayer_001(x)\n",
    "            x = F.relu(self.convlayer_002(x))\n",
    "            x = self.convlayer_003(x)\n",
    "            return x\n",
    "\n",
    "        def forward(self, data):\n",
    "            x = self._forward_features(data.x)\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = F.relu(self.fclayer_000(x))\n",
    "            x = self.fclayer_001(x)\n",
    "            return x\n",
    "  ```\n",
    "- The rest of the `Trainer` parameters can be used as explained already for graphs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.trainer:Device set to cpu.\n",
      "INFO:deeprankcore.trainer:No loss function provided, the default loss function for classif tasks is used: <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n",
      "INFO:deeprankcore.trainer:Training set loaded\n",
      "\n",
      "INFO:deeprankcore.trainer:Validation set loaded\n",
      "\n",
      "INFO:deeprankcore.trainer:class occurences: tensor([36., 36.])\n",
      "INFO:deeprankcore.trainer:class weights: tensor([0.5000, 0.5000])\n",
      "INFO:deeprankcore.trainer:Epoch 0:\n",
      "INFO:deeprankcore.trainer:training loss 1.7472169531716242 | time 4.757069110870361\n",
      "INFO:deeprankcore.trainer:validation loss 1.6783498922983806 | time 1.3219969272613525\n",
      "INFO:deeprankcore.trainer:Epoch 1:\n",
      "INFO:deeprankcore.trainer:training loss 1.162523587544759 | time 5.591736793518066\n",
      "INFO:deeprankcore.trainer:validation loss 0.6937174532148573 | time 1.2610390186309814\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 1.\n",
      "INFO:deeprankcore.trainer:Epoch 2:\n",
      "INFO:deeprankcore.trainer:training loss 0.6948328879144456 | time 5.621981143951416\n",
      "INFO:deeprankcore.trainer:validation loss 0.6937150888972812 | time 1.2283997535705566\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 2.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693717 --> 0.693715).\n",
      "INFO:deeprankcore.trainer:Epoch 3:\n",
      "INFO:deeprankcore.trainer:training loss 0.6944651206334432 | time 5.481665134429932\n",
      "INFO:deeprankcore.trainer:validation loss 0.6937079694535997 | time 1.1818928718566895\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 3.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693715 --> 0.693708).\n",
      "INFO:deeprankcore.trainer:Epoch 4:\n",
      "INFO:deeprankcore.trainer:training loss 0.6942722532484267 | time 5.562615871429443\n",
      "INFO:deeprankcore.trainer:validation loss 0.6937008566326566 | time 1.2187657356262207\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 4.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693708 --> 0.693701).\n",
      "INFO:deeprankcore.trainer:Epoch 5:\n",
      "INFO:deeprankcore.trainer:training loss 0.6941937936676873 | time 5.691114664077759\n",
      "INFO:deeprankcore.trainer:validation loss 0.6936942338943481 | time 1.2306058406829834\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 5.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693701 --> 0.693694).\n",
      "INFO:deeprankcore.trainer:Epoch 6:\n",
      "INFO:deeprankcore.trainer:training loss 0.6941367321544223 | time 5.555415153503418\n",
      "INFO:deeprankcore.trainer:validation loss 0.6936872005462646 | time 1.1971447467803955\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 6.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693694 --> 0.693687).\n",
      "INFO:deeprankcore.trainer:Epoch 7:\n",
      "INFO:deeprankcore.trainer:training loss 0.6940836442841424 | time 5.247174024581909\n",
      "INFO:deeprankcore.trainer:validation loss 0.6936804056167603 | time 1.1882836818695068\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 7.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693687 --> 0.693680).\n",
      "INFO:deeprankcore.trainer:Epoch 8:\n",
      "INFO:deeprankcore.trainer:training loss 0.6940302782588534 | time 5.3221588134765625\n",
      "INFO:deeprankcore.trainer:validation loss 0.6936737034055922 | time 1.2059710025787354\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 8.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693680 --> 0.693674).\n",
      "INFO:deeprankcore.trainer:Epoch 9:\n",
      "INFO:deeprankcore.trainer:training loss 0.6940279073185391 | time 5.3078789710998535\n",
      "INFO:deeprankcore.trainer:validation loss 0.69366764360004 | time 1.1947729587554932\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 9.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693674 --> 0.693668).\n",
      "INFO:deeprankcore.trainer:Epoch 10:\n",
      "INFO:deeprankcore.trainer:training loss 0.6940309471554227 | time 5.400295972824097\n",
      "INFO:deeprankcore.trainer:validation loss 0.6936612791485257 | time 1.2363560199737549\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 10.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693668 --> 0.693661).\n",
      "INFO:deeprankcore.trainer:Epoch 11:\n",
      "INFO:deeprankcore.trainer:training loss 0.6939985222286649 | time 5.84884786605835\n",
      "INFO:deeprankcore.trainer:validation loss 0.6936554047796462 | time 1.2304391860961914\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 11.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693661 --> 0.693655).\n",
      "INFO:deeprankcore.trainer:Epoch 12:\n",
      "INFO:deeprankcore.trainer:training loss 0.6935529841317071 | time 5.88683009147644\n",
      "INFO:deeprankcore.trainer:validation loss 0.6936492853694491 | time 1.2347452640533447\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 12.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693655 --> 0.693649).\n",
      "INFO:deeprankcore.trainer:Epoch 13:\n",
      "INFO:deeprankcore.trainer:training loss 0.6940382321675619 | time 6.020850896835327\n",
      "INFO:deeprankcore.trainer:validation loss 0.6936436891555786 | time 1.246901035308838\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 13.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693649 --> 0.693644).\n",
      "INFO:deeprankcore.trainer:Epoch 14:\n",
      "INFO:deeprankcore.trainer:training loss 0.6940393182966444 | time 5.870224952697754\n",
      "INFO:deeprankcore.trainer:validation loss 0.6936377286911011 | time 1.216705083847046\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 14.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693644 --> 0.693638).\n",
      "INFO:deeprankcore.trainer:Epoch 15:\n",
      "INFO:deeprankcore.trainer:training loss 0.693955229388343 | time 5.658677816390991\n",
      "INFO:deeprankcore.trainer:validation loss 0.6936324040095011 | time 1.2131011486053467\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 15.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693638 --> 0.693632).\n",
      "INFO:deeprankcore.trainer:Epoch 16:\n",
      "INFO:deeprankcore.trainer:training loss 0.693928857644399 | time 5.779132843017578\n",
      "INFO:deeprankcore.trainer:validation loss 0.6936267150772942 | time 1.251655101776123\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 16.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693632 --> 0.693627).\n",
      "INFO:deeprankcore.trainer:Epoch 17:\n",
      "INFO:deeprankcore.trainer:training loss 0.6934951610035367 | time 5.742820978164673\n",
      "INFO:deeprankcore.trainer:validation loss 0.6936211321089003 | time 1.2225570678710938\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 17.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693627 --> 0.693621).\n",
      "INFO:deeprankcore.trainer:Epoch 18:\n",
      "INFO:deeprankcore.trainer:training loss 0.6939395864804586 | time 5.7037999629974365\n",
      "INFO:deeprankcore.trainer:validation loss 0.6936156551043192 | time 1.2279727458953857\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 18.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693621 --> 0.693616).\n",
      "INFO:deeprankcore.trainer:Epoch 19:\n",
      "INFO:deeprankcore.trainer:training loss 0.6938799288537767 | time 5.4559571743011475\n",
      "INFO:deeprankcore.trainer:validation loss 0.693609979417589 | time 1.210676908493042\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 19.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693616 --> 0.693610).\n",
      "INFO:deeprankcore.trainer:Epoch 20:\n",
      "INFO:deeprankcore.trainer:training loss 0.6936315033170912 | time 5.352306842803955\n",
      "INFO:deeprankcore.trainer:validation loss 0.6936046547359891 | time 1.2130801677703857\n",
      "INFO:deeprankcore.trainer:Best model saved at epoch # 20.\n",
      "INFO:deeprankcore.trainer:Validation loss decreased (0.693610 --> 0.693605).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at epoch 20\n",
      "Total # of parameters: 122599\n",
      "Total # of trainable parameters: 122599\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD\n",
    "lr = 1e-3\n",
    "weight_decay = 0.001\n",
    "epochs = 20\n",
    "batch_size = 8\n",
    "earlystop_patience = 5\n",
    "earlystop_maxgap = 0.1\n",
    "min_epoch = 10\n",
    "\n",
    "trainer = Trainer(\n",
    "    neuralnet = CnnClassification,\n",
    "    dataset_train = dataset_train,\n",
    "    dataset_val = dataset_val,\n",
    "    dataset_test = dataset_test,\n",
    "    output_exporters = [HDF5OutputExporter(os.path.join(output_path, \"cnn_classif\"))]\n",
    ")\n",
    "\n",
    "trainer.configure_optimizers(optimizer, lr, weight_decay)\n",
    "\n",
    "trainer.train(\n",
    "    nepoch = epochs,\n",
    "    batch_size = batch_size,\n",
    "    earlystop_patience = earlystop_patience,\n",
    "    earlystop_maxgap = earlystop_maxgap,\n",
    "    min_epoch = min_epoch,\n",
    "    validate = True,\n",
    "    filename = os.path.join(output_path, \"cnn_classif\", \"model.pth.tar\"))\n",
    "\n",
    "epoch = trainer.epoch_saved_model\n",
    "print(f\"Model saved at epoch {epoch}\")\n",
    "pytorch_total_params = sum(p.numel() for p in trainer.model.parameters())\n",
    "print(f'Total # of parameters: {pytorch_total_params}')\n",
    "pytorch_trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
    "print(f'Total # of trainable parameters: {pytorch_trainable_params}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing\n",
    "\n",
    "And we can test the trained model on our `dataset_test`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:deeprankcore.trainer:Loading independent testing dataset...\n",
      "INFO:deeprankcore.trainer:Testing set loaded\n",
      "\n",
      "INFO:deeprankcore.trainer:testing loss 0.6927011013031006 | time 0.7682321071624756\n"
     ]
    }
   ],
   "source": [
    "trainer.test()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results visualization\n",
    "\n",
    "As for the GNN, we can finally inspect the results saved by `HDF5OutputExporter`, which can be found in `data/ppi/cnn_classif/` folder in the form of an HDF5 file, `output_exporter.hdf5`, together with the saved pre-trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phase</th>\n",
       "      <th>epoch</th>\n",
       "      <th>entry</th>\n",
       "      <th>output</th>\n",
       "      <th>target</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-128051</td>\n",
       "      <td>[0.09993983060121536, 0.90006023645401]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.747217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-146044</td>\n",
       "      <td>[0.017753494903445244, 0.9822465181350708]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.747217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-132553</td>\n",
       "      <td>[0.0869356021285057, 0.9130643606185913]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.747217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-119409</td>\n",
       "      <td>[0.004865861032158136, 0.995134174823761]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.747217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>training</td>\n",
       "      <td>0.0</td>\n",
       "      <td>residue-ppi:M-P:BA-126068</td>\n",
       "      <td>[0.08206354826688766, 0.9179364442825317]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.747217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      phase  epoch                      entry  \\\n",
       "0  training    0.0  residue-ppi:M-P:BA-128051   \n",
       "1  training    0.0  residue-ppi:M-P:BA-146044   \n",
       "2  training    0.0  residue-ppi:M-P:BA-132553   \n",
       "3  training    0.0  residue-ppi:M-P:BA-119409   \n",
       "4  training    0.0  residue-ppi:M-P:BA-126068   \n",
       "\n",
       "                                       output  target      loss  \n",
       "0     [0.09993983060121536, 0.90006023645401]     1.0  1.747217  \n",
       "1  [0.017753494903445244, 0.9822465181350708]     1.0  1.747217  \n",
       "2    [0.0869356021285057, 0.9130643606185913]     0.0  1.747217  \n",
       "3   [0.004865861032158136, 0.995134174823761]     0.0  1.747217  \n",
       "4   [0.08206354826688766, 0.9179364442825317]     1.0  1.747217  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_train = pd.read_hdf(os.path.join(output_path, \"cnn_classif\", \"output_exporter.hdf5\"), key=\"training\")\n",
    "output_test = pd.read_hdf(os.path.join(output_path, \"cnn_classif\", \"output_exporter.hdf5\"), key=\"testing\")\n",
    "output_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot also in this case the loss across the epochs for the training and the validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "phase=training<br>epoch=%{x}<br>loss=%{y}<extra></extra>",
         "legendgroup": "training",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "markers+lines",
         "name": "training",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20
         ],
         "xaxis": "x",
         "y": [
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.7472169531716242,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          1.162523587544759,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6948328879144456,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6944651206334432,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6942722532484267,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941937936676873,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6941367321544223,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940836442841424,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940302782588534,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940279073185391,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6940309471554227,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6939985222286649,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6935529841317071,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940382321675619,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.6940393182966444,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693955229388343,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.693928857644399,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6934951610035367,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6939395864804586,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6938799288537767,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912,
          0.6936315033170912
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "phase=validation<br>epoch=%{x}<br>loss=%{y}<extra></extra>",
         "legendgroup": "validation",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "markers+lines",
         "name": "validation",
         "showlegend": true,
         "type": "scattergl",
         "x": [
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          4,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          5,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          8,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          9,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          10,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          11,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          12,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          13,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          14,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          15,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          16,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          17,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          18,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          19,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20,
          20
         ],
         "xaxis": "x",
         "y": [
          1.6783498922983806,
          1.6783498922983806,
          1.6783498922983806,
          1.6783498922983806,
          1.6783498922983806,
          1.6783498922983806,
          1.6783498922983806,
          1.6783498922983806,
          1.6783498922983806,
          1.6783498922983806,
          1.6783498922983806,
          1.6783498922983806,
          1.6783498922983806,
          1.6783498922983806,
          1.6783498922983806,
          1.6783498922983806,
          1.6783498922983806,
          1.6783498922983806,
          0.6937174532148573,
          0.6937174532148573,
          0.6937174532148573,
          0.6937174532148573,
          0.6937174532148573,
          0.6937174532148573,
          0.6937174532148573,
          0.6937174532148573,
          0.6937174532148573,
          0.6937174532148573,
          0.6937174532148573,
          0.6937174532148573,
          0.6937174532148573,
          0.6937174532148573,
          0.6937174532148573,
          0.6937174532148573,
          0.6937174532148573,
          0.6937174532148573,
          0.6937150888972812,
          0.6937150888972812,
          0.6937150888972812,
          0.6937150888972812,
          0.6937150888972812,
          0.6937150888972812,
          0.6937150888972812,
          0.6937150888972812,
          0.6937150888972812,
          0.6937150888972812,
          0.6937150888972812,
          0.6937150888972812,
          0.6937150888972812,
          0.6937150888972812,
          0.6937150888972812,
          0.6937150888972812,
          0.6937150888972812,
          0.6937150888972812,
          0.6937079694535997,
          0.6937079694535997,
          0.6937079694535997,
          0.6937079694535997,
          0.6937079694535997,
          0.6937079694535997,
          0.6937079694535997,
          0.6937079694535997,
          0.6937079694535997,
          0.6937079694535997,
          0.6937079694535997,
          0.6937079694535997,
          0.6937079694535997,
          0.6937079694535997,
          0.6937079694535997,
          0.6937079694535997,
          0.6937079694535997,
          0.6937079694535997,
          0.6937008566326566,
          0.6937008566326566,
          0.6937008566326566,
          0.6937008566326566,
          0.6937008566326566,
          0.6937008566326566,
          0.6937008566326566,
          0.6937008566326566,
          0.6937008566326566,
          0.6937008566326566,
          0.6937008566326566,
          0.6937008566326566,
          0.6937008566326566,
          0.6937008566326566,
          0.6937008566326566,
          0.6937008566326566,
          0.6937008566326566,
          0.6937008566326566,
          0.6936942338943481,
          0.6936942338943481,
          0.6936942338943481,
          0.6936942338943481,
          0.6936942338943481,
          0.6936942338943481,
          0.6936942338943481,
          0.6936942338943481,
          0.6936942338943481,
          0.6936942338943481,
          0.6936942338943481,
          0.6936942338943481,
          0.6936942338943481,
          0.6936942338943481,
          0.6936942338943481,
          0.6936942338943481,
          0.6936942338943481,
          0.6936942338943481,
          0.6936872005462646,
          0.6936872005462646,
          0.6936872005462646,
          0.6936872005462646,
          0.6936872005462646,
          0.6936872005462646,
          0.6936872005462646,
          0.6936872005462646,
          0.6936872005462646,
          0.6936872005462646,
          0.6936872005462646,
          0.6936872005462646,
          0.6936872005462646,
          0.6936872005462646,
          0.6936872005462646,
          0.6936872005462646,
          0.6936872005462646,
          0.6936872005462646,
          0.6936804056167603,
          0.6936804056167603,
          0.6936804056167603,
          0.6936804056167603,
          0.6936804056167603,
          0.6936804056167603,
          0.6936804056167603,
          0.6936804056167603,
          0.6936804056167603,
          0.6936804056167603,
          0.6936804056167603,
          0.6936804056167603,
          0.6936804056167603,
          0.6936804056167603,
          0.6936804056167603,
          0.6936804056167603,
          0.6936804056167603,
          0.6936804056167603,
          0.6936737034055922,
          0.6936737034055922,
          0.6936737034055922,
          0.6936737034055922,
          0.6936737034055922,
          0.6936737034055922,
          0.6936737034055922,
          0.6936737034055922,
          0.6936737034055922,
          0.6936737034055922,
          0.6936737034055922,
          0.6936737034055922,
          0.6936737034055922,
          0.6936737034055922,
          0.6936737034055922,
          0.6936737034055922,
          0.6936737034055922,
          0.6936737034055922,
          0.69366764360004,
          0.69366764360004,
          0.69366764360004,
          0.69366764360004,
          0.69366764360004,
          0.69366764360004,
          0.69366764360004,
          0.69366764360004,
          0.69366764360004,
          0.69366764360004,
          0.69366764360004,
          0.69366764360004,
          0.69366764360004,
          0.69366764360004,
          0.69366764360004,
          0.69366764360004,
          0.69366764360004,
          0.69366764360004,
          0.6936612791485257,
          0.6936612791485257,
          0.6936612791485257,
          0.6936612791485257,
          0.6936612791485257,
          0.6936612791485257,
          0.6936612791485257,
          0.6936612791485257,
          0.6936612791485257,
          0.6936612791485257,
          0.6936612791485257,
          0.6936612791485257,
          0.6936612791485257,
          0.6936612791485257,
          0.6936612791485257,
          0.6936612791485257,
          0.6936612791485257,
          0.6936612791485257,
          0.6936554047796462,
          0.6936554047796462,
          0.6936554047796462,
          0.6936554047796462,
          0.6936554047796462,
          0.6936554047796462,
          0.6936554047796462,
          0.6936554047796462,
          0.6936554047796462,
          0.6936554047796462,
          0.6936554047796462,
          0.6936554047796462,
          0.6936554047796462,
          0.6936554047796462,
          0.6936554047796462,
          0.6936554047796462,
          0.6936554047796462,
          0.6936554047796462,
          0.6936492853694491,
          0.6936492853694491,
          0.6936492853694491,
          0.6936492853694491,
          0.6936492853694491,
          0.6936492853694491,
          0.6936492853694491,
          0.6936492853694491,
          0.6936492853694491,
          0.6936492853694491,
          0.6936492853694491,
          0.6936492853694491,
          0.6936492853694491,
          0.6936492853694491,
          0.6936492853694491,
          0.6936492853694491,
          0.6936492853694491,
          0.6936492853694491,
          0.6936436891555786,
          0.6936436891555786,
          0.6936436891555786,
          0.6936436891555786,
          0.6936436891555786,
          0.6936436891555786,
          0.6936436891555786,
          0.6936436891555786,
          0.6936436891555786,
          0.6936436891555786,
          0.6936436891555786,
          0.6936436891555786,
          0.6936436891555786,
          0.6936436891555786,
          0.6936436891555786,
          0.6936436891555786,
          0.6936436891555786,
          0.6936436891555786,
          0.6936377286911011,
          0.6936377286911011,
          0.6936377286911011,
          0.6936377286911011,
          0.6936377286911011,
          0.6936377286911011,
          0.6936377286911011,
          0.6936377286911011,
          0.6936377286911011,
          0.6936377286911011,
          0.6936377286911011,
          0.6936377286911011,
          0.6936377286911011,
          0.6936377286911011,
          0.6936377286911011,
          0.6936377286911011,
          0.6936377286911011,
          0.6936377286911011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936324040095011,
          0.6936267150772942,
          0.6936267150772942,
          0.6936267150772942,
          0.6936267150772942,
          0.6936267150772942,
          0.6936267150772942,
          0.6936267150772942,
          0.6936267150772942,
          0.6936267150772942,
          0.6936267150772942,
          0.6936267150772942,
          0.6936267150772942,
          0.6936267150772942,
          0.6936267150772942,
          0.6936267150772942,
          0.6936267150772942,
          0.6936267150772942,
          0.6936267150772942,
          0.6936211321089003,
          0.6936211321089003,
          0.6936211321089003,
          0.6936211321089003,
          0.6936211321089003,
          0.6936211321089003,
          0.6936211321089003,
          0.6936211321089003,
          0.6936211321089003,
          0.6936211321089003,
          0.6936211321089003,
          0.6936211321089003,
          0.6936211321089003,
          0.6936211321089003,
          0.6936211321089003,
          0.6936211321089003,
          0.6936211321089003,
          0.6936211321089003,
          0.6936156551043192,
          0.6936156551043192,
          0.6936156551043192,
          0.6936156551043192,
          0.6936156551043192,
          0.6936156551043192,
          0.6936156551043192,
          0.6936156551043192,
          0.6936156551043192,
          0.6936156551043192,
          0.6936156551043192,
          0.6936156551043192,
          0.6936156551043192,
          0.6936156551043192,
          0.6936156551043192,
          0.6936156551043192,
          0.6936156551043192,
          0.6936156551043192,
          0.693609979417589,
          0.693609979417589,
          0.693609979417589,
          0.693609979417589,
          0.693609979417589,
          0.693609979417589,
          0.693609979417589,
          0.693609979417589,
          0.693609979417589,
          0.693609979417589,
          0.693609979417589,
          0.693609979417589,
          0.693609979417589,
          0.693609979417589,
          0.693609979417589,
          0.693609979417589,
          0.693609979417589,
          0.693609979417589,
          0.6936046547359891,
          0.6936046547359891,
          0.6936046547359891,
          0.6936046547359891,
          0.6936046547359891,
          0.6936046547359891,
          0.6936046547359891,
          0.6936046547359891,
          0.6936046547359891,
          0.6936046547359891,
          0.6936046547359891,
          0.6936046547359891,
          0.6936046547359891,
          0.6936046547359891,
          0.6936046547359891,
          0.6936046547359891,
          0.6936046547359891,
          0.6936046547359891
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "height": 400,
        "legend": {
         "title": {
          "text": "phase"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "shapes": [
         {
          "line": {
           "color": "green",
           "dash": "dash",
           "width": 3
          },
          "type": "line",
          "x0": 20,
          "x1": 20,
          "xref": "x",
          "y0": 0,
          "y1": 1,
          "yref": "y domain"
         }
        ],
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Loss vs epochs - CNN training"
        },
        "width": 700,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Epoch #"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Loss"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.line(\n",
    "    output_train,\n",
    "    x='epoch',\n",
    "    y='loss',\n",
    "    color='phase',\n",
    "    markers=True)\n",
    "\n",
    "fig.add_vline(x=trainer.epoch_saved_model, line_width=3, line_dash=\"dash\", line_color=\"green\")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Epoch #',\n",
    "    yaxis_title='Loss',\n",
    "    title='Loss vs epochs - CNN training',\n",
    "    width=700, height=400,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some metrics of interest for classification tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics for training:\n",
      "AUC: 0.4\n",
      "Considering a threshold of 0.5\n",
      "- Precision: 0.5\n",
      "- Recall: 0.0\n",
      "- Accuracy: 0.5\n",
      "- F1: 0.1\n",
      "\n",
      "Metrics for validation:\n",
      "AUC: 0.5\n",
      "Considering a threshold of 0.5\n",
      "- Precision: 0.0\n",
      "- Recall: 0.0\n",
      "- Accuracy: 0.5\n",
      "- F1: 0.0\n",
      "\n",
      "Metrics for testing:\n",
      "AUC: 0.6\n",
      "Considering a threshold of 0.5\n",
      "- Precision: 0.0\n",
      "- Recall: 0.0\n",
      "- Accuracy: 0.5\n",
      "- F1: 0.0\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "df = pd.concat([output_train, output_test])\n",
    "df_plot = df[(df.epoch == trainer.epoch_saved_model) | ((df.epoch == trainer.epoch_saved_model) & (df.phase == 'testing'))]\n",
    "\n",
    "for idx, set in enumerate(['training', 'validation', 'testing']):\n",
    "    df_plot_phase = df_plot[(df_plot.phase == set)]\n",
    "    y_true = df_plot_phase.target\n",
    "    y_score = np.array(df_plot_phase.output.values.tolist())[:, 1]\n",
    "\n",
    "    print(f'\\nMetrics for {set}:')\n",
    "    fpr_roc, tpr_roc, thr_roc = roc_curve(y_true, y_score)\n",
    "    auc_score = auc(fpr_roc, tpr_roc)\n",
    "    print(f'AUC: {round(auc_score, 1)}')\n",
    "    print(f'Considering a threshold of {threshold}')\n",
    "    y_pred = (y_score > threshold)*1\n",
    "    print(f'- Precision: {round(precision_score(y_true, y_pred), 1)}')\n",
    "    print(f'- Recall: {round(recall_score(y_true, y_pred), 1)}')\n",
    "    print(f'- Accuracy: {round(accuracy_score(y_true, y_pred), 1)}')\n",
    "    print(f'- F1: {round(f1_score(y_true, y_pred), 1)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results appear to be less favorable for CNN compared to GNN, but it's important to note that the dataset used in this analysis is not sufficiently large to provide conclusive and reliable insights. Depending on your specific application, you might find regression, classification, GNNs, and/or CNNs to be valuable options. Feel free to choose the approach that best aligns with your particular problem!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprankcore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
