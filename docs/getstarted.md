# Get started

## Data mapping

For each protein-protein complex, a query can be created and added to the `QueryCollection` object, to be processed later on. Different types of queries exist, based on the molecular resolution needed:
- In a `ProteinProteinInterfaceResidueQuery` each node represents one amino acid residue
- In a `ProteinProteinInterfaceAtomicQuery` each node represents one atom within the amino acid residues.
A query takes as inputs:
- a `.pdb` file, representing the protein-protein structural complex
- the ids of the two chains composing the complex, and 
- the correspondent Position-Specific Scoring Matrices (PSSMs), in the form of `.pssm` files.

```python
from deeprankcore.query import QueryCollection, ProteinProteinInterfaceResidueQuery

queries = QueryCollection()

# Append data points
queries.add(ProteinProteinInterfaceResidueQuery(
    pdb_path = "tests/data/pdb/1ATN/1ATN_1w.pdb",
    chain_id1 = "A",
    chain_id2 = "B",
    targets = {
        "binary": 0
    },
    pssm_paths = {
        "A": "tests/data/pssm/1ATN/1ATN.A.pdb.pssm",
        "B": "tests/data/pssm/1ATN/1ATN.B.pdb.pssm"
    }
))
queries.add(ProteinProteinInterfaceResidueQuery(
    pdb_path = "tests/data/pdb/1ATN/1ATN_2w.pdb",
    chain_id1 = "A",
    chain_id2 = "B",
    targets = {
        "binary": 1
    },
    pssm_paths = {
        "A": "tests/data/pssm/1ATN/1ATN.A.pdb.pssm",
        "B": "tests/data/pssm/1ATN/1ATN.B.pdb.pssm"
    }
))
queries.add(ProteinProteinInterfaceResidueQuery(
    pdb_path = "tests/data/pdb/1ATN/1ATN_3w.pdb",
    chain_id1 = "A",
    chain_id2 = "B",
    targets = {
        "binary": 0
    },
    pssm_paths = {
        "A": "tests/data/pssm/1ATN/1ATN.A.pdb.pssm",
        "B": "tests/data/pssm/1ATN/1ATN.B.pdb.pssm"
    }
))

```

The user is free to implement a custom query class. Each implementation requires the `build` method to be present.

The queries can then be processed into 3D-graphs only or both 3D-graphs and 3D-grids, depending on which kind of network will be used later for training. 

```python
from deeprankcore.features import components, conservation, contact, exposure, irc, surfacearea
from deeprankcore.utils.grid import GridSettings, MapMethod

feature_modules = [components, conservation, contact, exposure, irc, surfacearea]

# Save data into 3D-graphs only
hdf5_paths = queries.process(
    "<output_folder>/<prefix_for_outputs>",
    feature_modules = feature_modules)

# Save data into 3D-graphs and 3D-grids
hdf5_paths = queries.process(
    "<output_folder>/<prefix_for_outputs>",
    feature_modules = feature_modules,
    grid_settings = GridSettings(
        # the number of points on the x, y, z edges of the cube
        points_counts = [20, 20, 20],
        # x, y, z sizes of the box in Å
        sizes = [1.0, 1.0, 1.0]),
    grid_map_method = MapMethod.GAUSSIAN)
```

## Data exploration

As representative example, the following is the HDF5 structure generated by the previous phase for `1ATN_1w.pdb`, so for one single graph, for the graph + grid case:

```bash
└── ppi-1ATN_1w:A-B
    |
    ├── edge_features
    │   ├── _index
    │   ├── _name
    │   ├── covalent
    │   ├── distance
    │   ├── electrostatic
    │   ├── same_chain
    │   └── vanderwaals
    |
    ├── node_features
    │   ├── _chain_id
    │   ├── _name
    │   ├── _position
    │   ├── bsa
    │   ├── hse
    │   ├── info_content
    │   ├── res_depth
    │   ├── pssm
    |   ├── ...
    |   └── sasa
    |
    ├── grid_points
    │   ├── center
    │   ├── x
    │   ├── y
    │   └── z
    |
    ├── mapped_features
    │   ├── _position_000
    │   ├── _position_001
    │   ├── _position_002
    │   ├── bsa
    │   ├── covalent
    │   ├── distance
    │   ├── electrostatic
    │   ├── hse_000
    |   ├── ...
    |   └── vanderwaals
    |
    └── target_values
        └── binary
```

This entry represents the interface between the two proteins contained in the `.pdb` file, at the residue level. `edge_features` and `node_features` are specific for the graph-like representation of the PPI, while `grid_points` and `mapped_features` refer to the grid mapped from the graph. Each data point generated by deeprankcore has the above structure, apart from the features and the target that are specified by the user. 

It is always a good practice to first explore the data, and then make decision about splitting them in training, test and validation sets. For this purpose, users can either use [HDF5View](https://www.hdfgroup.org/downloads/hdfview/), a visual tool written in Java for browsing and editing HDF5 files, or Python packages such as [h5py](https://docs.h5py.org/en/stable/). Few examples for the latter:

```python
import h5py

with h5py.File("<hdf5_path.hdf5>", "r") as hdf5:
    # List of all graphs in hdf5, each graph representing a ppi
    ids = list(hdf5.keys())
    # List of all node features
    node_features = list(hdf5[ids[0]]["node_features"]) 
    # List of all edge features
    edge_features = list(hdf5[ids[0]]["edge_features"])
    # List of all edge targets
    targets = list(hdf5[ids[0]]["target_values"])
    # BSA feature for ids[0], numpy.ndarray
    node_feat_polarity = hdf5[ids[0]]["node_features"]["bsa"][:] 
     # Electrostatic feature for ids[0], numpy.ndarray
    edge_feat_electrostatic = hdf5[ids[0]]["edge_features"]["electrostatic"][:]
```

## Datasets

Data can be split in sets implementing custom splits according to the specific application. Assuming that the training, validation and testing ids have been chosen (keys of the HDF5 file/s), then the `DeeprankDataset` objects can be defined.

### GraphDataset

For training GNNs the user can create a `GraphDataset` instance:

```python
from deeprankcore.dataset import GraphDataset

node_features = ["bsa", "res_depth", "hse", "info_content", "pssm"]
edge_features = ["distance"]
target = "binary"

# Creating GraphDataset objects
dataset_train = GraphDataset(
    hdf5_path = hdf5_paths,
    subset = train_ids, 
    node_features = node_features,
    edge_features = edge_features,
    target = target
)
dataset_val = GraphDataset(
    hdf5_path = hdf5_paths,
    subset = valid_ids, 
    node_features = node_features,
    edge_features = edge_features,
    target = target

)
dataset_test = GraphDataset(
    hdf5_path = hdf5_paths,
    subset = test_ids, 
    node_features = node_features,
    edge_features = edge_features,
    target = target
)
```

#### Transforming features

For the `GraphDataset` class it is possible to define a dictionary to indicate which transformations to apply to the features, being the transformations lambda functions and/or standardization. If `True`, standardization is applied after transformation, if the latter is present. Example:

```python
import numpy as np
from deeprankcore.dataset import GraphDataset

node_features = ["bsa", "res_depth", "hse", "info_content", "pssm"]
edge_features = ["distance"]
target = "binary"

features_transform = {
    'bsa': {'transform': lambda t: np.log(t+1), 'standardize': True},
    'electrostatic': {'transform': lambda t: np.sqrt(t), 'standardize': True},
    'hse': {'transform': lambda t: np.log(t+1), 'standardize': False}
}

dataset_train = GraphDataset(
    hdf5_path = hdf5_path,
    subset = train_ids, 
    node_features = node_features,
    edge_features = edge_features,
    target = target,
    features_transform = features_transform
)
```

An `all` key can be set for indicating to apply the same `standardize` and `transform` to all the features present in the dataset. Example:

```python
features_transform = {'all':
    {'transform': lambda t: np.log(t+1), 'standardize': True}
}

dataset_train = GraphDataset(
    hdf5_path = hdf5_path,
    subset = train_ids, 
    node_features = node_features,
    edge_features = edge_features,
    target = target,
    features_transform = features_transform
)
```

If `standardize` functionality is used, validation and testing sets need to know the interested features' means and standard deviations in order to use the same values for standardizing validation and testing features. This can be done using `train` and `dataset_train` parameters of the `GraphDataset` class. Example:

```python
features_transform = {'all':
    {'transform': lambda t: np.log(t+1), 'standardize': True}
}

# `train` defaults to `True`, and `dataset_train` defaults to `None`
dataset_train = GraphDataset(
    hdf5_path = hdf5_path,
    subset = train_ids, 
    node_features = node_features,
    edge_features = edge_features,
    target = target,
    features_transform = features_transform
)
dataset_val = GraphDataset(
    hdf5_path = hdf5_paths,
    subset = valid_ids, 
    node_features = node_features,
    edge_features = edge_features,
    target = target,
    train = False,
    dataset_train = dataset_train # dataset_train means and stds will be used

)
dataset_test = GraphDataset(
    hdf5_path = hdf5_paths,
    subset = test_ids, 
    node_features = node_features,
    edge_features = edge_features,
    target = target,
    train = False,
    dataset_train = dataset_train # dataset_train means and stds will be used
)
```

### GridDataset

For training CNNs the user can create a `GridDataset` instance:

```python
from deeprankcore.dataset import GridDataset

features = ["bsa", "res_depth", "hse", "info_content", "pssm", "distance"]
target = "binary"

# Creating GraphDataset objects
dataset_train = GridDataset(
    hdf5_path = hdf5_paths,
    subset = train_ids, 
    features = features,
    target = target
)
dataset_val = GridDataset(
    hdf5_path = hdf5_paths,
    subset = valid_ids, 
    features = features,
    target = target

)
dataset_test = GridDataset(
    hdf5_path = hdf5_paths,
    subset = test_ids, 
    features = features,
    target = target
)
```

## Training

Let's define a `Trainer` instance, using for example of the already existing `GINet`. Because `GINet` is a GNN, it requires a dataset instance of type `GraphDataset`.

```python
from deeprankcore.trainer import Trainer
from deeprankcore.neuralnets.gnn.naive_gnn import NaiveNetwork

trainer = Trainer(
    NaiveNetwork,
    dataset_train,
    dataset_val,
    dataset_test
)

```

The same can be done using a CNN, for example `CnnClassification`. Here a dataset instance of type `GridDataset` is required.

```python
from deeprankcore.trainer import Trainer
from deeprankcore.neuralnets.cnn.model3d import CnnClassification

trainer = Trainer(
    CnnClassification,
    dataset_train,
    dataset_val,
    dataset_test
)
```

By default, the `Trainer` class creates the folder `./output` for storing predictions information collected later on during training and testing. `HDF5OutputExporter` is the exporter used by default, but the user can specify any other implemented exporter or implement a custom one.

Optimizer (`torch.optim.Adam` by default) and loss function can be defined by using dedicated functions:

```python
import torch

trainer.configure_optimizers(torch.optim.Adamax, lr = 0.001, weight_decay = 1e-04)

```

Then the `Trainer` can be trained and tested; the best model in terms of validation loss is saved by default, and the user can modify so or indicate where to save it using the `train()` method parameter `filename`.

```python
trainer.train(
    nepoch = 50,
    batch_size = 64,
    validate = True,
    filename = "<my_folder/model.pth.tar>")
trainer.test()

```

### Results export and visualization

The user can specify a deeprankcore exporter or a custom one in `output_exporters` parameter of the Trainer class, together with the path where to save the results. Exporters are used for storing predictions information collected later on during training and testing. Example:

```python
from deeprankcore.trainer import Trainer
from deeprankcore.neuralnets.gnn.naive_gnn import NaiveNetwork
from deeprankcore.utils.exporters import HDF5OutputExporter

trainer = Trainer(
    NaiveNetwork,
    dataset_train,
    dataset_val,
    dataset_test,
    output_exporters = [HDF5OutputExporter("<output_folder_path>")]
)
```

By default, the `Trainer` class creates the folder `./output` and uses `HDF5OutputExporter`. In the latter case, results are saved in `output_exporter.hdf5` both during the training (`.train()`) and during the testing (`.test()`) phases. `output_exporter.hdf5` contains Groups which refer to each phase, e.g. `training` and `testing` if both are run, only one of them otherwise. Training phase includes validation results as well. The HDF5 file can then be read as a [Pandas Dataframe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html):

```python
import os
import pandas as pd

output_train = pd.read_hdf(os.path.join("<output_folder_path>", "output_exporter.hdf5"), key="training")
output_test = pd.read_hdf(os.path.join("<output_folder_path>", "output_exporter.hdf5"), key="testing")
```

The dataframes contain `phase`, `epoch`, `entry`, `output`, `target`, and `loss` columns, and can be easily used to visualize the results. 

Example for plotting training loss curves using [Plotly Express](https://plotly.com/python/plotly-express/):

```python
import plotly.express as px

fig = px.line(
    output_train,
    x='epoch',
    y='loss',
    color='phase',
    markers=True)

fig.update_layout(
    xaxis_title='Epoch #',
    yaxis_title='Loss',
    title='Loss vs epochs'
)
```

## h5x support

After installing  `h5xplorer`  (https://github.com/DeepRank/h5xplorer), you can execute the python file `deeprankcore/h5x/h5x.py` to explorer the connection graph used by deeprankcore. The context menu (right click on the name of the structure) allows to automatically plot the graphs using `plotly`.
